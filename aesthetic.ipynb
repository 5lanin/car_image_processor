{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "746e2a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import clip\n",
    "from transformers import CLIPProcessor\n",
    "from aesthetics_predictor import AestheticsPredictorV1\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class AestheticScorer:\n",
    "    \"\"\"\n",
    "    Robust aesthetic scorer with multiple fallback methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.device = device\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        self.clip_model = None\n",
    "        self.clip_preprocess = None\n",
    "\n",
    "        # Try to initialize the best available method\n",
    "        self._initialize_scorer()\n",
    "        self._initialize_sim_model()\n",
    "    \n",
    "    def _initialize_scorer(self):\n",
    "        \"\"\"Initialize the best available scoring method.\"\"\"\n",
    "\n",
    "        model_id = \"shunk031/aesthetics-predictor-v1-vit-large-patch14\"\n",
    "\n",
    "        self.model = AestheticsPredictorV1.from_pretrained(model_id)\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_id)\n",
    "\n",
    "        if self.device == \"cuda\":\n",
    "            self.model = self.model.to(self.device)\n",
    "\n",
    "    def _initialize_sim_model(self): \n",
    "        self.clip_model, self.clip_preprocess = clip.load(\"ViT-B/32\", device=self.device)\n",
    "        self.clip_model.eval()\n",
    "        \n",
    "        # Define aesthetic text prompts\n",
    "        self.positive_prompts = [\n",
    "            \"a beautiful, high quality professional image of a car\",\n",
    "            \"sharp, clear, well-composed image with good lighting\",\n",
    "            \"aesthetically pleasing image of a car\"\n",
    "        ]\n",
    "        \n",
    "        self.negative_prompts = [\n",
    "            \"blurry, low quality, poorly composed image\",\n",
    "            \"dark, unclear, amateur photograph\",\n",
    "            \"bad quality, unappealing photo\"\n",
    "        ]\n",
    "        \n",
    "        # Precompute text features\n",
    "        pos_tokens = clip.tokenize(self.positive_prompts).to(self.device)\n",
    "        neg_tokens = clip.tokenize(self.negative_prompts).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.pos_features = self.clip_model.encode_text(pos_tokens)\n",
    "            self.neg_features = self.clip_model.encode_text(neg_tokens)\n",
    "            \n",
    "            # Normalize features\n",
    "            self.pos_features = self.pos_features / self.pos_features.norm(dim=-1, keepdim=True)\n",
    "            self.neg_features = self.neg_features / self.neg_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "\n",
    "    def score(self, image_input: str | Image.Image) -> float:\n",
    "        \"\"\"\n",
    "        Score an image using the best available method.\n",
    "        \n",
    "        Args:\n",
    "            image_input: Path to the image file, or PIL Image \n",
    "            \n",
    "        Returns:\n",
    "            Aesthetic score from 0-10, Clip score from 0-10\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(image_input, str):\n",
    "            image = Image.open(image_input).convert(\"RGB\")\n",
    "        elif isinstance(image_input, Image.Image):\n",
    "            image = image_input\n",
    "        else:\n",
    "            raise ValueError(\"Invalid image input type.\")\n",
    "\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        if self.device == \"cuda\":\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad(): # or `torch.inference_model` in torch 1.9+\n",
    "            outputs = self.model(**inputs)\n",
    "        score = outputs.logits.item()\n",
    "\n",
    "        return score\n",
    "\n",
    "    def score_clip_similarity(self, image_input: str | Image.Image) -> float:\n",
    "        \"\"\"Score using CLIP similarity to aesthetic prompts.\"\"\"\n",
    "\n",
    "        if isinstance(image_input, str):\n",
    "            image = Image.open(image_input).convert(\"RGB\")\n",
    "        elif isinstance(image_input, Image.Image):\n",
    "            image = image_input\n",
    "        else:\n",
    "            raise ValueError(\"Invalid image input type.\")\n",
    "\n",
    "        image_embed = self.clip_preprocess(image).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_features = self.clip_model.encode_image(image_embed)\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Calculate similarities\n",
    "            pos_sim = (image_features @ self.pos_features.T).mean().item()\n",
    "            neg_sim = (image_features @ self.neg_features.T).mean().item()\n",
    "        \n",
    "        # Convert to 0-10 scale\n",
    "        # Similarity ranges from -1 to 1, we want higher pos_sim and lower neg_sim\n",
    "        score = ((pos_sim - neg_sim) + 1) * 5  # Maps [-1, 1] to [0, 10]\n",
    "        \n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f8276c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing aesthetic scorer...\n"
     ]
    }
   ],
   "source": [
    "scorer = AestheticScorer()\n",
    "    \n",
    "# url = \"https://github.com/shunk031/simple-aesthetics-predictor/blob/master/assets/a-photo-of-an-astronaut-riding-a-horse.png?raw=true\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "image_path = \"/mnt/damian/Projects/minRF/data/car_images/ASTONMARTIN_ASTON_MARTIN_Rapide_AMR_2017_0a437262.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e58f18cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of image: 7.462876319885254\n"
     ]
    }
   ],
   "source": [
    "score_laion = scorer.score(image_path)\n",
    "print(f\"Score of image: {score_laion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bced2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP similarity score of image: 5.225830078125\n"
     ]
    }
   ],
   "source": [
    "clip_sim = scorer.score_clip_similarity(image_path)\n",
    "print(f\"CLIP similarity score of image: {clip_sim}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
