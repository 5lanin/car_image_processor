{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Processing Debug Notebook\n",
    "\n",
    "This notebook systematically debugs the batch processing pipeline by breaking it down into individual steps.\n",
    "Each section will verify that the processing works correctly and identify where bugs are introduced.\n",
    "\n",
    "## Goal\n",
    "Find and fix the issues causing poor quality vehicle cutouts in batch processing while maintaining the same quality as single image processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup & Single Image Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Import SAM2\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "# Import our working single processor for baseline\n",
    "from enhanced_vehicle_processor import PreSAMSuperResolutionProcessor\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Vehicle classes\n",
    "VEHICLE_CLASSES = {2: 'car', 5: 'bus', 7: 'truck'}\n",
    "\n",
    "print(\"\\n‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test image setup\n",
    "test_image_path = \"/mnt/damian/Projects/car_data_scraper/images/autoevolution_renderings/article_230605/230605_reborn-ford-bronco-ii-morphs-ranger-ms-rt-dna-to-mix-and-match-with-suv-body-style_7_15.jpg\"\n",
    "\n",
    "# Verify test image exists\n",
    "if os.path.exists(test_image_path):\n",
    "    print(f\"‚úÖ Test image found: {Path(test_image_path).name}\")\n",
    "    \n",
    "    # Display original image\n",
    "    orig_img = Image.open(test_image_path)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(orig_img)\n",
    "    plt.title(f\"Original Test Image ({orig_img.size[0]}x{orig_img.size[1]})\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå Test image not found: {test_image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test batch image loading function\n",
    "print(\"Testing batch image loading...\")\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def load_images_batch(image_paths: List[str]) -> List[Tuple[np.ndarray, str]]:\n",
    "    \"\"\"Load multiple images efficiently\"\"\"\n",
    "    loaded_images = []\n",
    "    \n",
    "    def load_single_image(path: str) -> Optional[Tuple[np.ndarray, str]]:\n",
    "        try:\n",
    "            pil_image = Image.open(path).convert('RGB')\n",
    "            cv2_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n",
    "            return cv2_image, path\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load image {path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Use ThreadPoolExecutor for I/O optimization\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        future_to_path = {executor.submit(load_single_image, path): path for path in image_paths}\n",
    "        \n",
    "        for future in as_completed(future_to_path):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                loaded_images.append(result)\n",
    "    \n",
    "    return loaded_images\n",
    "\n",
    "# Load our test image using batch function\n",
    "test_images = load_images_batch([test_image_path])\n",
    "\n",
    "if test_images:\n",
    "    cv2_image, loaded_path = test_images[0]\n",
    "    print(f\"‚úÖ Image loaded successfully: {cv2_image.shape}\")\n",
    "    \n",
    "    # Display loaded image (convert back to RGB for display)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"Batch-loaded Image (should match original)\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Store cv2_image globally for later sections\n",
    "    print(\"Image stored as 'cv2_image' for use in subsequent sections\")\n",
    "else:\n",
    "    print(\"‚ùå Batch image loading failed\")\n",
    "    cv2_image = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test batch image loading function\n",
    "print(\"Testing batch image loading...\")\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def load_images_batch(image_paths: List[str]) -> List[Tuple[np.ndarray, str]]:\n",
    "    \"\"\"Load multiple images efficiently\"\"\"\n",
    "    loaded_images = []\n",
    "    \n",
    "    def load_single_image(path: str) -> Optional[Tuple[np.ndarray, str]]:\n",
    "        try:\n",
    "            pil_image = Image.open(path).convert('RGB')\n",
    "            cv2_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n",
    "            return cv2_image, path\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load image {path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Use ThreadPoolExecutor for I/O optimization\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        future_to_path = {executor.submit(load_single_image, path): path for path in image_paths}\n",
    "        \n",
    "        for future in as_completed(future_to_path):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                loaded_images.append(result)\n",
    "    \n",
    "    return loaded_images\n",
    "\n",
    "# Load our test image using batch function\n",
    "test_images = load_images_batch([test_image_path])\n",
    "\n",
    "if test_images:\n",
    "    cv2_image, loaded_path = test_images[0]\n",
    "    print(f\"‚úÖ Image loaded successfully: {cv2_image.shape}\")\n",
    "    \n",
    "    # Display loaded image (convert back to RGB for display)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"Batch-loaded Image (should match original)\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ùå Batch image loading failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test YOLO detection with batch method\n",
    "print(\"Testing YOLO detection...\")\n",
    "\n",
    "# Initialize YOLO model\n",
    "yolo_model = YOLO('yolo11x.pt').to('cuda')\n",
    "yolo_model.eval()\n",
    "\n",
    "def expand_bbox(box: np.ndarray, image_shape: tuple, expansion_factor: float = 0.25) -> np.ndarray:\n",
    "    \"\"\"Expand bounding box by expansion factor while keeping within bounds\"\"\"\n",
    "    x1, y1, x2, y2 = box\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "    \n",
    "    expand_w = width * expansion_factor\n",
    "    expand_h = height * expansion_factor\n",
    "    \n",
    "    x1 = max(0, x1 - expand_w / 2)\n",
    "    y1 = max(0, y1 - expand_h / 2)\n",
    "    x2 = min(image_shape[1], x2 + expand_w / 2)\n",
    "    y2 = min(image_shape[0], y2 + expand_h / 2)\n",
    "    \n",
    "    return np.array([x1, y1, x2, y2])\n",
    "\n",
    "def batch_detect_vehicles(image: np.ndarray) -> Dict:\n",
    "    \"\"\"Detect vehicles in image\"\"\"\n",
    "    results = yolo_model(source=image, conf=0.25, verbose=False)\n",
    "    detections = sv.Detections.from_ultralytics(results[0])\n",
    "    \n",
    "    # Filter for vehicle classes\n",
    "    vehicle_mask = np.isin(detections.class_id, list(VEHICLE_CLASSES.keys()))\n",
    "    \n",
    "    if vehicle_mask.any():\n",
    "        # Expand bounding boxes\n",
    "        expanded_boxes = []\n",
    "        for box in detections.xyxy[vehicle_mask]:\n",
    "            expanded_box = expand_bbox(box, image.shape[:2], 0.25)\n",
    "            expanded_boxes.append(expanded_box)\n",
    "        \n",
    "        return {\n",
    "            'boxes': np.array(expanded_boxes),\n",
    "            'original_boxes': detections.xyxy[vehicle_mask],\n",
    "            'confidences': detections.confidence[vehicle_mask],\n",
    "            'class_ids': detections.class_id[vehicle_mask],\n",
    "            'labels': [VEHICLE_CLASSES[cid] for cid in detections.class_id[vehicle_mask]]\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'boxes': np.array([]), 'original_boxes': np.array([]),\n",
    "            'confidences': np.array([]), 'class_ids': np.array([]), 'labels': []\n",
    "        }\n",
    "\n",
    "# Run YOLO detection ONLY if cv2_image exists\n",
    "if cv2_image is not None:\n",
    "    detections = batch_detect_vehicles(cv2_image)\n",
    "    \n",
    "    print(f\"Detection results:\")\n",
    "    print(f\"  Vehicles found: {len(detections['boxes'])}\")\n",
    "    if len(detections['boxes']) > 0:\n",
    "        for i, (conf, label) in enumerate(zip(detections['confidences'], detections['labels'])):\n",
    "            print(f\"  Vehicle {i+1}: {label} (confidence: {conf:.3f})\")\n",
    "            print(f\"    Original bbox: {detections['original_boxes'][i]}\")\n",
    "            print(f\"    Expanded bbox: {detections['boxes'][i]}\")\n",
    "    \n",
    "    # Visualize detections\n",
    "    if len(detections['boxes']) > 0:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "        \n",
    "        # Show original bboxes\n",
    "        display_image = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB)\n",
    "        ax1.imshow(display_image)\n",
    "        ax1.set_title('Original YOLO Detections')\n",
    "        \n",
    "        for box, conf, label in zip(detections['original_boxes'], detections['confidences'], detections['labels']):\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='red', facecolor='none')\n",
    "            ax1.add_patch(rect)\n",
    "            ax1.text(x1, y1-10, f'{label}: {conf:.2f}', bbox=dict(boxstyle='round', facecolor='red', alpha=0.7),\n",
    "                    fontsize=10, color='white')\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        # Show expanded bboxes\n",
    "        ax2.imshow(display_image)\n",
    "        ax2.set_title('Expanded Bounding Boxes (25%)')\n",
    "        \n",
    "        for box, conf, label in zip(detections['boxes'], detections['confidences'], detections['labels']):\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='blue', facecolor='none')\n",
    "            ax2.add_patch(rect)\n",
    "            ax2.text(x1, y1-10, f'{label}: {conf:.2f}', bbox=dict(boxstyle='round', facecolor='blue', alpha=0.7),\n",
    "                    fontsize=10, color='white')\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ YOLO detection working correctly\")\n",
    "        print(\"Detections stored as 'detections' for use in subsequent sections\")\n",
    "    else:\n",
    "        print(\"‚ùå No vehicles detected\")\n",
    "        detections = None\n",
    "else:\n",
    "    print(\"‚ùå No image loaded - skipping detection\")\n",
    "    detections = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Region Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test region extraction from bounding boxes\n",
    "print(\"Testing region extraction...\")\n",
    "\n",
    "if len(detections['boxes']) > 0:\n",
    "    for i, (box, conf, label) in enumerate(zip(detections['boxes'], detections['confidences'], detections['labels'])):\n",
    "        print(f\"\\nExtracting region for vehicle {i+1}: {label}\")\n",
    "        \n",
    "        # Extract region directly from bbox\n",
    "        x1, y1, x2, y2 = box.astype(int)\n",
    "        print(f\"  Bbox coordinates: ({x1}, {y1}, {x2}, {y2})\")\n",
    "        print(f\"  Region size: {x2-x1} x {y2-y1}\")\n",
    "        \n",
    "        # Extract the region\n",
    "        region = cv2_image[y1:y2, x1:x2]\n",
    "        print(f\"  Extracted region shape: {region.shape}\")\n",
    "        \n",
    "        if region.size > 0:\n",
    "            # Convert to RGB for display\n",
    "            region_rgb = cv2.cvtColor(region, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Display extracted region\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.imshow(region_rgb)\n",
    "            plt.title(f\"Extracted Region {i+1}: {label}\\n\"\n",
    "                     f\"Size: {region.shape[1]}x{region.shape[0]}, Confidence: {conf:.3f}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "            # Save extracted region for comparison\n",
    "            region_path = f\"./debug_output/extracted_region_{i+1}_{label}.png\"\n",
    "            Image.fromarray(region_rgb).save(region_path)\n",
    "            print(f\"  ‚úÖ Saved extracted region: {region_path}\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå Empty region extracted - check bbox coordinates\")\n",
    "else:\n",
    "    print(\"‚ùå No vehicles to extract regions from\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Region Standardization & Letterboxing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test region standardization with smart padding and dual bbox tracking (ENHANCED VERSION)\n",
    "print(\"Testing region standardization with smart padding and dual bbox tracking...\")\n",
    "\n",
    "def extract_context_region(image: np.ndarray, bbox: np.ndarray, context_factor: float = 0.5) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extract a larger context region around bbox for smart padding\n",
    "    \n",
    "    Args:\n",
    "        image: Full original image\n",
    "        bbox: Bounding box [x1, y1, x2, y2]  \n",
    "        context_factor: Additional context as fraction of bbox size\n",
    "        \n",
    "    Returns:\n",
    "        context_region: Extracted context region\n",
    "        context_bbox: Coordinates of context region in original image\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = bbox.astype(int)\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "    \n",
    "    # Add context padding\n",
    "    context_w = int(width * context_factor)\n",
    "    context_h = int(height * context_factor)\n",
    "    \n",
    "    # Calculate context region bounds\n",
    "    ctx_x1 = max(0, x1 - context_w)\n",
    "    ctx_y1 = max(0, y1 - context_h)\n",
    "    ctx_x2 = min(image.shape[1], x2 + context_w)\n",
    "    ctx_y2 = min(image.shape[0], y2 + context_h)\n",
    "    \n",
    "    context_region = image[ctx_y1:ctx_y2, ctx_x1:ctx_x2]\n",
    "    context_bbox = np.array([ctx_x1, ctx_y1, ctx_x2, ctx_y2])\n",
    "    \n",
    "    return context_region, context_bbox\n",
    "\n",
    "def standardize_region_with_smart_padding(\n",
    "    image: np.ndarray,           # Full original image\n",
    "    original_bbox: np.ndarray,   # Tight YOLO detection\n",
    "    expanded_bbox: np.ndarray,   # Expanded for context  \n",
    "    target_size: int = 800\n",
    ") -> Tuple[np.ndarray, Dict]:\n",
    "    \"\"\"\n",
    "    ENHANCED: Standardize region with smart padding and dual bbox tracking\n",
    "    \n",
    "    Key improvements:\n",
    "    1. Smart padding using original image content instead of black borders\n",
    "    2. Track both original and expanded bbox coordinates in 800x800 space\n",
    "    3. Enable precise SAM2 prompting with original bbox only\n",
    "    \n",
    "    Args:\n",
    "        image: Full original image\n",
    "        original_bbox: Tight YOLO detection bbox\n",
    "        expanded_bbox: Expanded bbox for extraction context\n",
    "        target_size: Target standardized size (800x800)\n",
    "        \n",
    "    Returns:\n",
    "        standardized_region: 800x800 image with smart padding\n",
    "        transform_info: Comprehensive coordinate mapping information\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the expanded region (for context)\n",
    "    ex_x1, ex_y1, ex_x2, ex_y2 = expanded_bbox.astype(int)\n",
    "    expanded_region = image[ex_y1:ex_y2, ex_x1:ex_x2]\n",
    "    \n",
    "    print(f\"  Expanded region size: {expanded_region.shape[1]}x{expanded_region.shape[0]}\")\n",
    "    \n",
    "    # Try to get larger context for smart padding\n",
    "    context_region, context_bbox = extract_context_region(image, expanded_bbox, context_factor=0.5)\n",
    "    print(f\"  Context region size: {context_region.shape[1]}x{context_region.shape[0]}\")\n",
    "    \n",
    "    # Calculate scale for expanded region to fit in target_size\n",
    "    eh, ew = expanded_region.shape[:2]\n",
    "    scale = target_size / max(eh, ew)\n",
    "    new_eh = int(eh * scale)\n",
    "    new_ew = int(ew * scale)\n",
    "    \n",
    "    print(f\"  Scale factor: {scale:.3f}\")\n",
    "    print(f\"  Scaled expanded size: {new_ew}x{new_eh}\")\n",
    "    \n",
    "    # Resize expanded region\n",
    "    resized_expanded = cv2.resize(expanded_region, (new_ew, new_eh), interpolation=cv2.INTER_LANCZOS4)\n",
    "    \n",
    "    # Create target_size x target_size canvas\n",
    "    standardized = np.zeros((target_size, target_size, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Calculate centering offsets for expanded region\n",
    "    y_offset = (target_size - new_eh) // 2\n",
    "    x_offset = (target_size - new_ew) // 2\n",
    "    \n",
    "    print(f\"  Centering offsets: x={x_offset}, y={y_offset}\")\n",
    "    \n",
    "    # SMART PADDING: Fill borders with context when possible\n",
    "    if context_region.shape[0] > expanded_region.shape[0] or context_region.shape[1] > expanded_region.shape[1]:\n",
    "        # Scale context region to same scale\n",
    "        ch, cw = context_region.shape[:2]\n",
    "        new_ch = int(ch * scale)\n",
    "        new_cw = int(cw * scale)\n",
    "        \n",
    "        if new_ch <= target_size and new_cw <= target_size:\n",
    "            resized_context = cv2.resize(context_region, (new_cw, new_ch), interpolation=cv2.INTER_LANCZOS4)\n",
    "            \n",
    "            # Center context region\n",
    "            ctx_y_offset = (target_size - new_ch) // 2\n",
    "            ctx_x_offset = (target_size - new_cw) // 2\n",
    "            \n",
    "            # Place context first (as background)\n",
    "            if ctx_y_offset >= 0 and ctx_x_offset >= 0:\n",
    "                standardized[ctx_y_offset:ctx_y_offset+new_ch, ctx_x_offset:ctx_x_offset+new_cw] = resized_context\n",
    "                print(f\"  Applied smart padding with context background\")\n",
    "            else:\n",
    "                print(f\"  Context too large, using black padding\")\n",
    "    \n",
    "    # Place resized expanded region on top (this is the main content)\n",
    "    standardized[y_offset:y_offset+new_eh, x_offset:x_offset+new_ew] = resized_expanded\n",
    "    \n",
    "    # CRITICAL: Calculate where original tight bbox maps to in 800x800 space\n",
    "    # Original bbox relative to expanded bbox\n",
    "    orig_x1, orig_y1, orig_x2, orig_y2 = original_bbox.astype(int)\n",
    "    \n",
    "    # Relative position within expanded region\n",
    "    rel_x1 = orig_x1 - ex_x1\n",
    "    rel_y1 = orig_y1 - ex_y1  \n",
    "    rel_x2 = orig_x2 - ex_x1\n",
    "    rel_y2 = orig_y2 - ex_y1\n",
    "    \n",
    "    # Scale and offset to 800x800 space\n",
    "    orig_800_x1 = x_offset + int(rel_x1 * scale)\n",
    "    orig_800_y1 = y_offset + int(rel_y1 * scale)\n",
    "    orig_800_x2 = x_offset + int(rel_x2 * scale)\n",
    "    orig_800_y2 = y_offset + int(rel_y2 * scale)\n",
    "    \n",
    "    # Ensure bounds are within target_size\n",
    "    orig_800_x1 = max(0, min(target_size, orig_800_x1))\n",
    "    orig_800_y1 = max(0, min(target_size, orig_800_y1))\n",
    "    orig_800_x2 = max(0, min(target_size, orig_800_x2))\n",
    "    orig_800_y2 = max(0, min(target_size, orig_800_y2))\n",
    "    \n",
    "    original_bbox_800 = np.array([orig_800_x1, orig_800_y1, orig_800_x2, orig_800_y2])\n",
    "    \n",
    "    print(f\"  Original bbox in 800x800: ({orig_800_x1}, {orig_800_y1}, {orig_800_x2}, {orig_800_y2})\")\n",
    "    \n",
    "    # Store comprehensive transformation info\n",
    "    transform_info = {\n",
    "        # Expanded region info (for backward compatibility)\n",
    "        'expanded_size': (ew, eh),  # (width, height)\n",
    "        'scale': scale,\n",
    "        'scaled_size': (new_ew, new_eh),  # (width, height)\n",
    "        'offset': (x_offset, y_offset),  # (x, y) - where expanded region is placed\n",
    "        \n",
    "        # NEW: Original bbox tracking\n",
    "        'original_bbox_800': original_bbox_800,  # Original bbox coordinates in 800x800 space\n",
    "        'original_bbox_orig': original_bbox,     # Original bbox in source image\n",
    "        'expanded_bbox_orig': expanded_bbox,     # Expanded bbox in source image\n",
    "        \n",
    "        # Canvas info\n",
    "        'letterbox_size': target_size,\n",
    "        'used_smart_padding': context_region.shape[0] > expanded_region.shape[0] or context_region.shape[1] > expanded_region.shape[1]\n",
    "    }\n",
    "    \n",
    "    return standardized, transform_info\n",
    "\n",
    "# Test enhanced standardization on extracted regions\n",
    "if len(detections['boxes']) > 0:\n",
    "    for i, (expanded_box, original_box, conf, label) in enumerate(zip(\n",
    "        detections['boxes'], \n",
    "        detections['original_boxes'],  # Use original tight detections  \n",
    "        detections['confidences'], \n",
    "        detections['labels']\n",
    "    )):\n",
    "        print(f\"\\n=== Enhanced Standardization for Region {i+1}: {label} ===\")\n",
    "        \n",
    "        # Extract using expanded bbox, but track original bbox\n",
    "        standardized_region, transform_info = standardize_region_with_smart_padding(\n",
    "            cv2_image, original_box, expanded_box, 800\n",
    "        )\n",
    "        \n",
    "        # Display comparison with bbox tracking\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "        \n",
    "        # Original image with both bboxes\n",
    "        display_image = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB)\n",
    "        axes[0,0].imshow(display_image)\n",
    "        axes[0,0].set_title(f\"Original Image with Dual Bboxes\")\n",
    "        \n",
    "        # Draw original tight bbox (red)\n",
    "        orig_x1, orig_y1, orig_x2, orig_y2 = original_box\n",
    "        orig_rect = patches.Rectangle((orig_x1, orig_y1), orig_x2-orig_x1, orig_y2-orig_y1, \n",
    "                                     linewidth=2, edgecolor='red', facecolor='none')\n",
    "        axes[0,0].add_patch(orig_rect)\n",
    "        axes[0,0].text(orig_x1, orig_y1-10, 'Original YOLO', \n",
    "                      bbox=dict(boxstyle='round', facecolor='red', alpha=0.7),\n",
    "                      fontsize=10, color='white')\n",
    "        \n",
    "        # Draw expanded bbox (blue)\n",
    "        exp_x1, exp_y1, exp_x2, exp_y2 = expanded_box\n",
    "        exp_rect = patches.Rectangle((exp_x1, exp_y1), exp_x2-exp_x1, exp_y2-exp_y1, \n",
    "                                    linewidth=2, edgecolor='blue', facecolor='none')\n",
    "        axes[0,0].add_patch(exp_rect)\n",
    "        axes[0,0].text(exp_x1, exp_y1-30, 'Expanded (Context)', \n",
    "                      bbox=dict(boxstyle='round', facecolor='blue', alpha=0.7),\n",
    "                      fontsize=10, color='white')\n",
    "        axes[0,0].axis('off')\n",
    "        \n",
    "        # Extracted expanded region\n",
    "        exp_region = cv2_image[int(exp_y1):int(exp_y2), int(exp_x1):int(exp_x2)]\n",
    "        axes[0,1].imshow(cv2.cvtColor(exp_region, cv2.COLOR_BGR2RGB))\n",
    "        axes[0,1].set_title(f\"Extracted Expanded Region\\n{exp_region.shape[1]}x{exp_region.shape[0]}\")\n",
    "        axes[0,1].axis('off')\n",
    "        \n",
    "        # Standardized region with smart padding\n",
    "        axes[1,0].imshow(cv2.cvtColor(standardized_region, cv2.COLOR_BGR2RGB))\n",
    "        axes[1,0].set_title(f\"Standardized 800x800\\\\nSmart Padding: {transform_info['used_smart_padding']}\")\n",
    "        \n",
    "        # Draw expanded content area (blue)\n",
    "        x_off, y_off = transform_info['offset']\n",
    "        new_w, new_h = transform_info['scaled_size']\n",
    "        exp_rect_800 = patches.Rectangle((x_off, y_off), new_w, new_h, \n",
    "                                       linewidth=2, edgecolor='blue', facecolor='none', alpha=0.7)\n",
    "        axes[1,0].add_patch(exp_rect_800)\n",
    "        \n",
    "        # Draw original bbox mapping (red) - THIS IS KEY FOR SAM2!\n",
    "        orig_bbox_800 = transform_info['original_bbox_800']\n",
    "        ox1, oy1, ox2, oy2 = orig_bbox_800\n",
    "        orig_rect_800 = patches.Rectangle((ox1, oy1), ox2-ox1, oy2-oy1, \n",
    "                                        linewidth=3, edgecolor='red', facecolor='none')\n",
    "        axes[1,0].add_patch(orig_rect_800)\n",
    "        axes[1,0].text(ox1, oy1-10, 'Original‚ÜíSAM2', \n",
    "                      bbox=dict(boxstyle='round', facecolor='red', alpha=0.7),\n",
    "                      fontsize=12, color='white')\n",
    "        axes[1,0].axis('off')\n",
    "        \n",
    "        # Zoom on original bbox area in 800x800\n",
    "        margin = 50\n",
    "        zoom_x1 = max(0, ox1 - margin)\n",
    "        zoom_y1 = max(0, oy1 - margin)\n",
    "        zoom_x2 = min(800, ox2 + margin)\n",
    "        zoom_y2 = min(800, oy2 + margin)\n",
    "        \n",
    "        zoomed_region = standardized_region[int(zoom_y1):int(zoom_y2), int(zoom_x1):int(zoom_x2)]\n",
    "        axes[1,1].imshow(cv2.cvtColor(zoomed_region, cv2.COLOR_BGR2RGB))\n",
    "        axes[1,1].set_title(f\"Zoomed Original Bbox Area\\\\nSAM2 will focus here\")\n",
    "        \n",
    "        # Draw the original bbox within zoom\n",
    "        zoom_ox1 = ox1 - zoom_x1\n",
    "        zoom_oy1 = oy1 - zoom_y1\n",
    "        zoom_ox2 = ox2 - zoom_x1\n",
    "        zoom_oy2 = oy2 - zoom_y1\n",
    "        zoom_rect = patches.Rectangle((zoom_ox1, zoom_oy1), zoom_ox2-zoom_ox1, zoom_oy2-zoom_oy1, \n",
    "                                    linewidth=2, edgecolor='red', facecolor='none')\n",
    "        axes[1,1].add_patch(zoom_rect)\n",
    "        axes[1,1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Save enhanced standardized region\n",
    "        std_path = f\"./debug_output/enhanced_standardized_{i+1}_{label}.png\"\n",
    "        Image.fromarray(cv2.cvtColor(standardized_region, cv2.COLOR_BGR2RGB)).save(std_path)\n",
    "        print(f\"  ‚úÖ Saved enhanced standardized region: {std_path}\")\n",
    "        \n",
    "        # Print comprehensive transform info\n",
    "        print(f\"  üìä Transform Summary:\")\n",
    "        print(f\"    - Expanded region: {transform_info['expanded_size']} ‚Üí {transform_info['scaled_size']}\")\n",
    "        print(f\"    - Placement offset: {transform_info['offset']}\")\n",
    "        print(f\"    - Scale factor: {transform_info['scale']:.3f}\")\n",
    "        print(f\"    - Original bbox (800x800): {list(transform_info['original_bbox_800'])}\")\n",
    "        print(f\"    - Smart padding used: {transform_info['used_smart_padding']}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No regions to standardize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: SAM2 Bbox Prompt Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SAM2 bbox prompt generation with CORRECTED original bbox coordinates\n",
    "print(\"Testing SAM2 bbox prompt generation with original bbox mapping...\")\n",
    "\n",
    "# Initialize SAM2\n",
    "checkpoint_path = \"/mnt/damian/Projects/sam2/checkpoints/sam2.1_hiera_large.pt\"\n",
    "sam2_config = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "sam2_model = build_sam2(sam2_config, checkpoint_path, device='cuda')\n",
    "sam2_predictor = SAM2ImagePredictor(sam2_model)\n",
    "\n",
    "if len(detections['boxes']) > 0:\n",
    "    for i, (expanded_box, original_box, conf, label) in enumerate(zip(\n",
    "        detections['boxes'], \n",
    "        detections['original_boxes'],\n",
    "        detections['confidences'], \n",
    "        detections['labels']\n",
    "    )):\n",
    "        print(f\"\\n=== CORRECTED SAM2 Prompt for Region {i+1}: {label} ===\")\n",
    "        \n",
    "        # Use enhanced standardization with dual bbox tracking\n",
    "        standardized_region, transform_info = standardize_region_with_smart_padding(\n",
    "            cv2_image, original_box, expanded_box, 800\n",
    "        )\n",
    "        \n",
    "        # üî• KEY FIX: Use original bbox coordinates in 800x800 space for SAM2 prompt\n",
    "        # Instead of the expanded content area which covers too much\n",
    "        original_bbox_800 = transform_info['original_bbox_800']\n",
    "        bbox_prompt = np.array([original_bbox_800])  # This is the tight vehicle area only!\n",
    "        \n",
    "        print(f\"  üîß OLD APPROACH (WRONG): Would use expanded content area\")\n",
    "        print(f\"      Expanded content: offset={transform_info['offset']}, size={transform_info['scaled_size']}\")\n",
    "        print(f\"      Would give SAM2: [{transform_info['offset'][0]}, {transform_info['offset'][1]}, {transform_info['offset'][0] + transform_info['scaled_size'][0]}, {transform_info['offset'][1] + transform_info['scaled_size'][1]}]\")\n",
    "        \n",
    "        print(f\"  ‚úÖ NEW APPROACH (CORRECT): Using original tight bbox\")\n",
    "        print(f\"      Original bbox in 800x800: {list(original_bbox_800)}\")\n",
    "        print(f\"      SAM2 bbox prompt: {bbox_prompt[0]}\")\n",
    "        \n",
    "        # Calculate the difference in area\n",
    "        old_area = transform_info['scaled_size'][0] * transform_info['scaled_size'][1]\n",
    "        new_area = (original_bbox_800[2] - original_bbox_800[0]) * (original_bbox_800[3] - original_bbox_800[1])\n",
    "        area_reduction = (1 - new_area / old_area) * 100\n",
    "        \n",
    "        print(f\"  üìä Prompt area reduction: {area_reduction:.1f}% (focusing on vehicle only)\")\n",
    "        \n",
    "        # Visualize the corrected prompting approach\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
    "        \n",
    "        # Standardized region with smart padding\n",
    "        standardized_rgb = cv2.cvtColor(standardized_region, cv2.COLOR_BGR2RGB)\n",
    "        axes[0].imshow(standardized_rgb)\n",
    "        axes[0].set_title(f\"Standardized Region {i+1}\\\\nSmart Padding: {transform_info['used_smart_padding']}\")\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Show OLD (wrong) approach - expanded content area\n",
    "        axes[1].imshow(standardized_rgb)\n",
    "        axes[1].set_title(f\"OLD Approach (WRONG)\\\\nExpanded Content Area Prompt\")\n",
    "        \n",
    "        # Draw old expanded content prompt (blue - wrong)\n",
    "        x_off, y_off = transform_info['offset']\n",
    "        new_w, new_h = transform_info['scaled_size']\n",
    "        old_rect = patches.Rectangle((x_off, y_off), new_w, new_h, \n",
    "                                   linewidth=3, edgecolor='blue', facecolor='blue', alpha=0.3)\n",
    "        axes[1].add_patch(old_rect)\n",
    "        axes[1].text(x_off, y_off-10, 'OLD: Expanded Area\\\\n(SAM2 segments ALL)', \n",
    "                    bbox=dict(boxstyle='round', facecolor='blue', alpha=0.8),\n",
    "                    fontsize=10, color='white')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        # Show NEW (correct) approach - original tight bbox\n",
    "        axes[2].imshow(standardized_rgb)\n",
    "        axes[2].set_title(f\"NEW Approach (CORRECT)\\\\nOriginal Tight Bbox Prompt\")\n",
    "        \n",
    "        # Draw new original bbox prompt (red - correct)\n",
    "        ox1, oy1, ox2, oy2 = original_bbox_800\n",
    "        new_rect = patches.Rectangle((ox1, oy1), ox2-ox1, oy2-oy1, \n",
    "                                   linewidth=3, edgecolor='red', facecolor='red', alpha=0.3)\n",
    "        axes[2].add_patch(new_rect)\n",
    "        axes[2].text(ox1, oy1-10, 'NEW: Vehicle Only\\\\n(SAM2 segments VEHICLE)', \n",
    "                    bbox=dict(boxstyle='round', facecolor='red', alpha=0.8),\n",
    "                    fontsize=10, color='white')\n",
    "        \n",
    "        # Also show the old area in background for comparison\n",
    "        old_rect_bg = patches.Rectangle((x_off, y_off), new_w, new_h, \n",
    "                                      linewidth=1, edgecolor='blue', facecolor='none', alpha=0.5, linestyle='--')\n",
    "        axes[2].add_patch(old_rect_bg)\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Save corrected prompt visualization\n",
    "        prompt_path = f\"./debug_output/corrected_sam2_prompt_{i+1}_{label}.png\"\n",
    "        plt.savefig(prompt_path, bbox_inches='tight', dpi=150)\n",
    "        print(f\"  ‚úÖ Saved corrected SAM2 prompt visualization: {prompt_path}\")\n",
    "        \n",
    "        # Show zoomed comparison of the two approaches\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "        \n",
    "        # Zoom on expanded area (old approach)\n",
    "        margin = 20\n",
    "        zoom_exp_x1 = max(0, x_off - margin)\n",
    "        zoom_exp_y1 = max(0, y_off - margin)\n",
    "        zoom_exp_x2 = min(800, x_off + new_w + margin)\n",
    "        zoom_exp_y2 = min(800, y_off + new_h + margin)\n",
    "        \n",
    "        zoomed_exp = standardized_region[int(zoom_exp_y1):int(zoom_exp_y2), int(zoom_exp_x1):int(zoom_exp_x2)]\n",
    "        ax1.imshow(cv2.cvtColor(zoomed_exp, cv2.COLOR_BGR2RGB))\n",
    "        ax1.set_title(\"OLD: SAM2 would segment\\\\nentire expanded area\")\n",
    "        \n",
    "        # Draw expanded area within zoom\n",
    "        zoom_exp_rect = patches.Rectangle((x_off - zoom_exp_x1, y_off - zoom_exp_y1), \n",
    "                                        new_w, new_h, \n",
    "                                        linewidth=2, edgecolor='blue', facecolor='blue', alpha=0.3)\n",
    "        ax1.add_patch(zoom_exp_rect)\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        # Zoom on original bbox area (new approach)\n",
    "        zoom_orig_x1 = max(0, ox1 - margin)\n",
    "        zoom_orig_y1 = max(0, oy1 - margin)\n",
    "        zoom_orig_x2 = min(800, ox2 + margin)\n",
    "        zoom_orig_y2 = min(800, oy2 + margin)\n",
    "        \n",
    "        zoomed_orig = standardized_region[int(zoom_orig_y1):int(zoom_orig_y2), int(zoom_orig_x1):int(zoom_orig_x2)]\n",
    "        ax2.imshow(cv2.cvtColor(zoomed_orig, cv2.COLOR_BGR2RGB))\n",
    "        ax2.set_title(\"NEW: SAM2 will segment\\\\njust the vehicle\")\n",
    "        \n",
    "        # Draw original bbox within zoom\n",
    "        zoom_orig_rect = patches.Rectangle((ox1 - zoom_orig_x1, oy1 - zoom_orig_y1), \n",
    "                                         ox2-ox1, oy2-oy1, \n",
    "                                         linewidth=2, edgecolor='red', facecolor='red', alpha=0.3)\n",
    "        ax2.add_patch(zoom_orig_rect)\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"  üéØ This corrected prompt should result in much better vehicle segmentation!\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No regions for SAM2 prompt generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: SAM2 Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SAM2 segmentation with CORRECTED prompts and smart padding  \n",
    "print(\"Testing SAM2 segmentation with corrected prompts...\")\n",
    "\n",
    "if len(detections['boxes']) > 0:\n",
    "    for i, (expanded_box, original_box, conf, label) in enumerate(zip(\n",
    "        detections['boxes'], \n",
    "        detections['original_boxes'],\n",
    "        detections['confidences'], \n",
    "        detections['labels']\n",
    "    )):\n",
    "        print(f\"\\n=== CORRECTED SAM2 Segmentation for Region {i+1}: {label} ===\")\n",
    "        \n",
    "        # Use enhanced standardization with smart padding and dual bbox tracking\n",
    "        standardized_region, transform_info = standardize_region_with_smart_padding(\n",
    "            cv2_image, original_box, expanded_box, 800\n",
    "        )\n",
    "        \n",
    "        # Convert to RGB for SAM2\n",
    "        standardized_rgb = cv2.cvtColor(standardized_region, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Set image for SAM2\n",
    "        sam2_predictor.set_image(standardized_rgb)\n",
    "        \n",
    "        # üî• KEY FIX: Use original bbox coordinates for SAM2 prompt (not expanded area)\n",
    "        original_bbox_800 = transform_info['original_bbox_800']\n",
    "        bbox_prompt = np.array([original_bbox_800])\n",
    "        \n",
    "        print(f\"  Using CORRECTED bbox prompt: {bbox_prompt[0]}\")\n",
    "        print(f\"  Prompt covers vehicle only (not expanded context)\")\n",
    "        \n",
    "        try:\n",
    "            # Run SAM2 segmentation with corrected prompt\n",
    "            mask_result, quality_scores, _ = sam2_predictor.predict(\n",
    "                point_coords=None,\n",
    "                point_labels=None,\n",
    "                box=bbox_prompt,\n",
    "                multimask_output=False,\n",
    "                return_logits=False,\n",
    "            )\n",
    "            \n",
    "            if len(mask_result) > 0:\n",
    "                mask = mask_result[0]\n",
    "                score = quality_scores[0]\n",
    "                \n",
    "                print(f\"  ‚úÖ SAM2 segmentation successful with corrected prompt\")\n",
    "                print(f\"  Mask shape: {mask.shape}\")\n",
    "                print(f\"  Quality score: {score:.3f}\")\n",
    "                print(f\"  Mask coverage: {np.sum(mask) / mask.size * 100:.1f}%\")\n",
    "                \n",
    "                # Calculate mask coverage within the original bbox area vs expanded area\n",
    "                ox1, oy1, ox2, oy2 = original_bbox_800.astype(int)\n",
    "                mask_in_original_bbox = mask[oy1:oy2, ox1:ox2]\n",
    "                original_bbox_coverage = np.sum(mask_in_original_bbox) / mask_in_original_bbox.size * 100\n",
    "                \n",
    "                x_off, y_off = transform_info['offset']\n",
    "                new_w, new_h = transform_info['scaled_size']\n",
    "                mask_in_expanded = mask[y_off:y_off+new_h, x_off:x_off+new_w]\n",
    "                expanded_coverage = np.sum(mask_in_expanded) / mask_in_expanded.size * 100\n",
    "                \n",
    "                print(f\"  üìä Mask analysis:\")\n",
    "                print(f\"    - Coverage in original bbox area: {original_bbox_coverage:.1f}%\")\n",
    "                print(f\"    - Coverage in expanded area: {expanded_coverage:.1f}%\")\n",
    "                print(f\"    - Focus ratio (orig/expanded): {original_bbox_coverage/max(expanded_coverage,1):.2f}\")\n",
    "                \n",
    "                # Visualize CORRECTED segmentation result with comparison\n",
    "                fig, axes = plt.subplots(2, 4, figsize=(32, 16))\n",
    "                \n",
    "                # Top row: Input and prompt visualization\n",
    "                axes[0,0].imshow(standardized_rgb)\n",
    "                axes[0,0].set_title(f\"Input: Standardized Region\\\\nSmart Padding: {transform_info['used_smart_padding']}\")\n",
    "                axes[0,0].axis('off')\n",
    "                \n",
    "                # Show both prompt areas for comparison\n",
    "                axes[0,1].imshow(standardized_rgb)\n",
    "                axes[0,1].set_title(\"OLD vs NEW Prompts\")\n",
    "                \n",
    "                # Draw OLD expanded prompt (blue, dashed)\n",
    "                old_rect = patches.Rectangle((x_off, y_off), new_w, new_h, \n",
    "                                           linewidth=2, edgecolor='blue', facecolor='none', \n",
    "                                           linestyle='--', alpha=0.7)\n",
    "                axes[0,1].add_patch(old_rect)\n",
    "                axes[0,1].text(x_off, y_off-10, 'OLD: Expanded', \n",
    "                              bbox=dict(boxstyle='round', facecolor='blue', alpha=0.7),\n",
    "                              fontsize=9, color='white')\n",
    "                \n",
    "                # Draw NEW original prompt (red, solid)\n",
    "                new_rect = patches.Rectangle((ox1, oy1), ox2-ox1, oy2-oy1, \n",
    "                                           linewidth=3, edgecolor='red', facecolor='none')\n",
    "                axes[0,1].add_patch(new_rect)\n",
    "                axes[0,1].text(ox1, oy1-10, 'NEW: Original', \n",
    "                              bbox=dict(boxstyle='round', facecolor='red', alpha=0.7),\n",
    "                              fontsize=9, color='white')\n",
    "                axes[0,1].axis('off')\n",
    "                \n",
    "                # Generated mask\n",
    "                axes[0,2].imshow(mask, cmap='gray')\n",
    "                axes[0,2].set_title(f\"Generated Mask\\\\nScore: {score:.3f}\")\n",
    "                axes[0,2].axis('off')\n",
    "                \n",
    "                # Mask overlay on input\n",
    "                overlay = standardized_rgb.copy().astype(np.float32)\n",
    "                mask_bool = mask.astype(bool)\n",
    "                overlay[mask_bool] = overlay[mask_bool] * 0.6 + np.array([0, 255, 0]) * 0.4\n",
    "                overlay = overlay.astype(np.uint8)\n",
    "                axes[0,3].imshow(overlay)\n",
    "                axes[0,3].set_title(f\"Mask Overlay\\\\nCoverage: {np.sum(mask) / mask.size * 100:.1f}%\")\n",
    "                axes[0,3].axis('off')\n",
    "                \n",
    "                # Bottom row: Focused analysis\n",
    "                # Zoom on original bbox area\n",
    "                margin = 30\n",
    "                zoom_x1 = max(0, ox1 - margin)\n",
    "                zoom_y1 = max(0, oy1 - margin)\n",
    "                zoom_x2 = min(800, ox2 + margin)\n",
    "                zoom_y2 = min(800, oy2 + margin)\n",
    "                \n",
    "                zoom_img = standardized_rgb[zoom_y1:zoom_y2, zoom_x1:zoom_x2]\n",
    "                zoom_mask = mask[zoom_y1:zoom_y2, zoom_x1:zoom_x2]\n",
    "                \n",
    "                axes[1,0].imshow(zoom_img)\n",
    "                axes[1,0].set_title(\"Zoomed: Original Bbox Area\")\n",
    "                # Draw bbox within zoom\n",
    "                zoom_rect = patches.Rectangle((ox1-zoom_x1, oy1-zoom_y1), ox2-ox1, oy2-oy1, \n",
    "                                            linewidth=2, edgecolor='red', facecolor='none')\n",
    "                axes[1,0].add_patch(zoom_rect)\n",
    "                axes[1,0].axis('off')\n",
    "                \n",
    "                axes[1,1].imshow(zoom_mask, cmap='gray')\n",
    "                axes[1,1].set_title(\"Zoomed: Mask in Bbox Area\")\n",
    "                axes[1,1].axis('off')\n",
    "                \n",
    "                # Zoomed overlay\n",
    "                zoom_overlay = zoom_img.copy().astype(np.float32)\n",
    "                zoom_mask_bool = zoom_mask.astype(bool)\n",
    "                zoom_overlay[zoom_mask_bool] = zoom_overlay[zoom_mask_bool] * 0.6 + np.array([0, 255, 0]) * 0.4\n",
    "                zoom_overlay = zoom_overlay.astype(np.uint8)\n",
    "                axes[1,2].imshow(zoom_overlay)\n",
    "                axes[1,2].set_title(\"Zoomed: Overlay Result\")\n",
    "                axes[1,2].axis('off')\n",
    "                \n",
    "                # Quality assessment\n",
    "                if original_bbox_coverage > 70:\n",
    "                    quality_text = \"‚úÖ EXCELLENT\\\\nHigh vehicle coverage\"\n",
    "                    quality_color = 'green'\n",
    "                elif original_bbox_coverage > 50:\n",
    "                    quality_text = \"‚úÖ GOOD\\\\nDecent vehicle coverage\" \n",
    "                    quality_color = 'orange'\n",
    "                else:\n",
    "                    quality_text = \"‚ö†Ô∏è POOR\\\\nLow vehicle coverage\"\n",
    "                    quality_color = 'red'\n",
    "                \n",
    "                axes[1,3].text(0.5, 0.5, f\"Segmentation Quality\\\\n\\\\n{quality_text}\\\\n\\\\nBbox Coverage: {original_bbox_coverage:.1f}%\\\\nQuality Score: {score:.3f}\", \n",
    "                              ha='center', va='center', transform=axes[1,3].transAxes,\n",
    "                              bbox=dict(boxstyle='round', facecolor=quality_color, alpha=0.3),\n",
    "                              fontsize=12)\n",
    "                axes[1,3].axis('off')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Save corrected segmentation visualization\n",
    "                mask_path = f\"./debug_output/corrected_sam2_mask_{i+1}_{label}.png\"\n",
    "                plt.savefig(mask_path, bbox_inches='tight', dpi=150)\n",
    "                print(f\"  ‚úÖ Saved corrected mask visualization: {mask_path}\")\n",
    "                \n",
    "                # Quality assessment\n",
    "                if original_bbox_coverage > 70 and score > 0.8:\n",
    "                    print(f\"  üéØ EXCELLENT: High-quality vehicle segmentation achieved!\")\n",
    "                elif original_bbox_coverage > 50 and score > 0.7:\n",
    "                    print(f\"  ‚úÖ GOOD: Decent vehicle segmentation quality\")\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è NEEDS IMPROVEMENT: Segmentation quality could be better\")\n",
    "                    \n",
    "            else:\n",
    "                print(f\"  ‚ùå SAM2 failed to generate mask with corrected prompt\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå SAM2 segmentation failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "                \n",
    "else:\n",
    "    print(\"‚ùå No regions for SAM2 segmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Mask Application & Coordinate Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CORRECTED mask application and coordinate mapping with smart padding\n",
    "print(\"Testing CORRECTED mask application with smart padding and dual bbox tracking...\")\n",
    "\n",
    "if len(detections['boxes']) > 0:\n",
    "    for i, (expanded_box, original_box, conf, label) in enumerate(zip(\n",
    "        detections['boxes'], \n",
    "        detections['original_boxes'],\n",
    "        detections['confidences'], \n",
    "        detections['labels']\n",
    "    )):\n",
    "        print(f\"\\n=== CORRECTED Mask Application for Region {i+1}: {label} ===\")\n",
    "        \n",
    "        # Use enhanced standardization with smart padding and dual bbox tracking\n",
    "        standardized_region, transform_info = standardize_region_with_smart_padding(\n",
    "            cv2_image, original_box, expanded_box, 800\n",
    "        )\n",
    "        \n",
    "        # Convert to RGB for SAM2\n",
    "        standardized_rgb = cv2.cvtColor(standardized_region, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Set image for SAM2 and get mask with CORRECTED prompt\n",
    "        sam2_predictor.set_image(standardized_rgb)\n",
    "        \n",
    "        # üî• Use CORRECTED original bbox coordinates for SAM2 prompt\n",
    "        original_bbox_800 = transform_info['original_bbox_800']\n",
    "        bbox_prompt = np.array([original_bbox_800])\n",
    "        \n",
    "        try:\n",
    "            mask_result, quality_scores, _ = sam2_predictor.predict(\n",
    "                point_coords=None,\n",
    "                point_labels=None,\n",
    "                box=bbox_prompt,\n",
    "                multimask_output=False,\n",
    "                return_logits=False,\n",
    "            )\n",
    "            \n",
    "            if len(mask_result) > 0:\n",
    "                mask_800 = mask_result[0]  # Mask in 800x800 space\n",
    "                score = quality_scores[0]\n",
    "                \n",
    "                print(f\"  ‚úÖ SAM2 segmentation successful with corrected prompt\")\n",
    "                print(f\"  Quality score: {score:.3f}\")\n",
    "                \n",
    "                # ENHANCED: Map mask back with smart coordinate handling\n",
    "                print(f\"  Mapping mask from 800x800 back to original region...\")\n",
    "                \n",
    "                # Extract mask for the EXPANDED region (for context preservation)\n",
    "                x_off, y_off = transform_info['offset']\n",
    "                new_w, new_h = transform_info['scaled_size']\n",
    "                mask_expanded_content = mask_800[y_off:y_off+new_h, x_off:x_off+new_w]\n",
    "                print(f\"  Expanded content mask shape: {mask_expanded_content.shape}\")\n",
    "                \n",
    "                # Resize mask back to original EXPANDED region size\n",
    "                ex_x1, ex_y1, ex_x2, ex_y2 = expanded_box.astype(int)\n",
    "                expanded_region = cv2_image[ex_y1:ex_y2, ex_x1:ex_x2]\n",
    "                orig_eh, orig_ew = expanded_region.shape[:2]\n",
    "                \n",
    "                mask_expanded_original = cv2.resize(mask_expanded_content.astype(np.uint8), \n",
    "                                                  (orig_ew, orig_eh), \n",
    "                                                  interpolation=cv2.INTER_NEAREST)\n",
    "                mask_expanded_original = mask_expanded_original.astype(bool)\n",
    "                print(f\"  Resized mask shape: {mask_expanded_original.shape}\")\n",
    "                print(f\"  Original expanded region shape: {expanded_region.shape[:2]}\")\n",
    "                \n",
    "                # Apply mask to expanded region (with smart background)\n",
    "                expanded_region_rgb = cv2.cvtColor(expanded_region, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Create RGBA image from expanded region\n",
    "                rgba_array = np.concatenate([\n",
    "                    expanded_region_rgb,\n",
    "                    np.ones((expanded_region_rgb.shape[0], expanded_region_rgb.shape[1], 1), dtype=np.uint8) * 255\n",
    "                ], axis=2)\n",
    "                \n",
    "                # Apply mask to alpha channel\n",
    "                rgba_array[:, :, 3] = (mask_expanded_original * 255).astype(np.uint8)\n",
    "                \n",
    "                # Convert to PIL image\n",
    "                masked_image = Image.fromarray(rgba_array, 'RGBA')\n",
    "                \n",
    "                # Calculate mask quality metrics\n",
    "                mask_coverage_800 = np.sum(mask_800) / mask_800.size * 100\n",
    "                mask_coverage_expanded = np.sum(mask_expanded_original) / mask_expanded_original.size * 100\n",
    "                \n",
    "                # Focus analysis: Check how much mask is in original bbox vs expanded area\n",
    "                orig_x1, orig_y1, orig_x2, orig_y2 = original_box.astype(int)\n",
    "                \n",
    "                # Map original bbox to expanded region coordinates\n",
    "                rel_orig_x1 = orig_x1 - ex_x1\n",
    "                rel_orig_y1 = orig_y1 - ex_y1\n",
    "                rel_orig_x2 = orig_x2 - ex_x1\n",
    "                rel_orig_y2 = orig_y2 - ex_y1\n",
    "                \n",
    "                # Ensure bounds are within expanded region\n",
    "                rel_orig_x1 = max(0, min(orig_ew, rel_orig_x1))\n",
    "                rel_orig_y1 = max(0, min(orig_eh, rel_orig_y1))\n",
    "                rel_orig_x2 = max(0, min(orig_ew, rel_orig_x2))\n",
    "                rel_orig_y2 = max(0, min(orig_eh, rel_orig_y2))\n",
    "                \n",
    "                if rel_orig_x2 > rel_orig_x1 and rel_orig_y2 > rel_orig_y1:\n",
    "                    mask_in_original_area = mask_expanded_original[rel_orig_y1:rel_orig_y2, rel_orig_x1:rel_orig_x2]\n",
    "                    original_area_coverage = np.sum(mask_in_original_area) / mask_in_original_area.size * 100\n",
    "                else:\n",
    "                    original_area_coverage = 0\n",
    "                \n",
    "                print(f\"  üìä Mask Coverage Analysis:\")\n",
    "                print(f\"    - Overall mask coverage: {mask_coverage_expanded:.1f}%\")\n",
    "                print(f\"    - Coverage in original vehicle area: {original_area_coverage:.1f}%\")\n",
    "                print(f\"    - Vehicle focus ratio: {original_area_coverage / max(mask_coverage_expanded, 1):.2f}\")\n",
    "                \n",
    "                # Visualize the CORRECTED mask application process\n",
    "                fig, axes = plt.subplots(3, 3, figsize=(24, 24))\n",
    "                \n",
    "                # Row 1: Input processing\n",
    "                axes[0,0].imshow(cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB))\n",
    "                axes[0,0].set_title(\"Original Image\")\n",
    "                \n",
    "                # Draw both bboxes\n",
    "                orig_rect = patches.Rectangle((orig_x1, orig_y1), orig_x2-orig_x1, orig_y2-orig_y1, \n",
    "                                            linewidth=2, edgecolor='red', facecolor='none')\n",
    "                exp_rect = patches.Rectangle((ex_x1, ex_y1), ex_x2-ex_x1, ex_y2-ex_y1, \n",
    "                                           linewidth=2, edgecolor='blue', facecolor='none')\n",
    "                axes[0,0].add_patch(orig_rect)\n",
    "                axes[0,0].add_patch(exp_rect)\n",
    "                axes[0,0].text(orig_x1, orig_y1-10, 'Original', \n",
    "                              bbox=dict(boxstyle='round', facecolor='red', alpha=0.7),\n",
    "                              fontsize=9, color='white')\n",
    "                axes[0,0].text(ex_x1, ex_y1-30, 'Expanded', \n",
    "                              bbox=dict(boxstyle='round', facecolor='blue', alpha=0.7),\n",
    "                              fontsize=9, color='white')\n",
    "                axes[0,0].axis('off')\n",
    "                \n",
    "                axes[0,1].imshow(expanded_region_rgb)\n",
    "                axes[0,1].set_title(f\"Extracted Expanded Region\\\\n{expanded_region.shape[1]}x{expanded_region.shape[0]}\")\n",
    "                axes[0,1].axis('off')\n",
    "                \n",
    "                axes[0,2].imshow(standardized_rgb)\n",
    "                axes[0,2].set_title(f\"Standardized 800x800\\\\nSmart Padding: {transform_info['used_smart_padding']}\")\n",
    "                \n",
    "                # Draw original bbox in 800x800 space (what SAM2 sees)\n",
    "                ox1, oy1, ox2, oy2 = original_bbox_800\n",
    "                sam2_rect = patches.Rectangle((ox1, oy1), ox2-ox1, oy2-oy1, \n",
    "                                            linewidth=3, edgecolor='red', facecolor='none')\n",
    "                axes[0,2].add_patch(sam2_rect)\n",
    "                axes[0,2].text(ox1, oy1-10, 'SAM2 Prompt', \n",
    "                              bbox=dict(boxstyle='round', facecolor='red', alpha=0.7),\n",
    "                              fontsize=9, color='white')\n",
    "                axes[0,2].axis('off')\n",
    "                \n",
    "                # Row 2: SAM2 processing\n",
    "                axes[1,0].imshow(mask_800, cmap='gray')\n",
    "                axes[1,0].set_title(f\"SAM2 Mask (800x800)\\\\nScore: {score:.3f}\")\n",
    "                axes[1,0].axis('off')\n",
    "                \n",
    "                axes[1,1].imshow(mask_expanded_content, cmap='gray')\n",
    "                axes[1,1].set_title(f\"Extracted Mask Content\\\\n{mask_expanded_content.shape[1]}x{mask_expanded_content.shape[0]}\")\n",
    "                axes[1,1].axis('off')\n",
    "                \n",
    "                axes[1,2].imshow(mask_expanded_original, cmap='gray')\n",
    "                axes[1,2].set_title(f\"Resized to Expanded Region\\\\n{mask_expanded_original.shape[1]}x{mask_expanded_original.shape[0]}\")\n",
    "                axes[1,2].axis('off')\n",
    "                \n",
    "                # Row 3: Final results\n",
    "                axes[2,0].imshow(masked_image)\n",
    "                axes[2,0].set_title(f\"Final Masked Result\\\\n(with smart background)\")\n",
    "                axes[2,0].axis('off')\n",
    "                \n",
    "                # Show focus area analysis\n",
    "                if rel_orig_x2 > rel_orig_x1 and rel_orig_y2 > rel_orig_y1:\n",
    "                    focus_area = rgba_array[rel_orig_y1:rel_orig_y2, rel_orig_x1:rel_orig_x2]\n",
    "                    if focus_area.size > 0:\n",
    "                        axes[2,1].imshow(focus_area)\n",
    "                        axes[2,1].set_title(f\"Original Vehicle Area\\\\nCoverage: {original_area_coverage:.1f}%\")\n",
    "                    else:\n",
    "                        axes[2,1].text(0.5, 0.5, 'Original area\\\\nnot available', \n",
    "                                      ha='center', va='center', transform=axes[2,1].transAxes)\n",
    "                else:\n",
    "                    axes[2,1].text(0.5, 0.5, 'Original area\\\\nnot available', \n",
    "                                  ha='center', va='center', transform=axes[2,1].transAxes)\n",
    "                axes[2,1].axis('off')\n",
    "                \n",
    "                # Quality assessment\n",
    "                if original_area_coverage > 70 and score > 0.8:\n",
    "                    quality_text = \"üéØ EXCELLENT\\nHigh-quality segmentation\\nStrong vehicle focus\"\n",
    "                    quality_color = 'green'\n",
    "                elif original_area_coverage > 50 and score > 0.7:\n",
    "                    quality_text = \"‚úÖ GOOD\\nDecent segmentation\\nReasonable vehicle focus\"\n",
    "                    quality_color = 'orange'\n",
    "                else:\n",
    "                    quality_text = \"‚ö†Ô∏è NEEDS IMPROVEMENT\\nPoor segmentation quality\\nLow vehicle focus\"\n",
    "                    quality_color = 'red'\n",
    "                \n",
    "                axes[2,2].text(0.5, 0.5, f\"CORRECTED Approach\\\\nQuality Assessment\\\\n\\\\n{quality_text}\\\\n\\\\nVehicle Coverage: {original_area_coverage:.1f}%\\\\nOverall Coverage: {mask_coverage_expanded:.1f}%\\\\nQuality Score: {score:.3f}\", \n",
    "                              ha='center', va='center', transform=axes[2,2].transAxes,\n",
    "                              bbox=dict(boxstyle='round', facecolor=quality_color, alpha=0.3),\n",
    "                              fontsize=10)\n",
    "                axes[2,2].axis('off')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Save corrected masked result\n",
    "                masked_path = f\"./debug_output/corrected_masked_result_{i+1}_{label}.png\"\n",
    "                masked_image.save(masked_path)\n",
    "                print(f\"  ‚úÖ Saved corrected masked result: {masked_path}\")\n",
    "                \n",
    "                # Final quality check\n",
    "                if original_area_coverage > 70 and score > 0.8:\n",
    "                    print(f\"  üéØ CORRECTED APPROACH SUCCESS: Excellent vehicle segmentation!\")\n",
    "                elif original_area_coverage > 50:\n",
    "                    print(f\"  ‚úÖ CORRECTED APPROACH GOOD: Decent vehicle segmentation\")\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è CORRECTED APPROACH: Still needs refinement\")\n",
    "                    \n",
    "                print(f\"  üìà Expected improvement: Tighter vehicle focus, better boundaries\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"  ‚ùå SAM2 failed to generate mask\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error in corrected mask application: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "                \n",
    "else:\n",
    "    print(\"‚ùå No regions for corrected mask application\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CORRECTED final output generation with smart padding and dual bbox tracking\n",
    "print(\"Testing CORRECTED final output generation...\")\n",
    "\n",
    "def create_final_output(masked_image: Image.Image, target_size: int = 512) -> Optional[Image.Image]:\n",
    "    \"\"\"Create final 512x512 output from masked image\"\"\"\n",
    "    try:\n",
    "        # Find bounding box of non-transparent pixels\n",
    "        alpha = np.array(masked_image.getchannel('A'))\n",
    "        coords = np.argwhere(alpha > 0)\n",
    "        \n",
    "        if len(coords) == 0:\n",
    "            return None\n",
    "        \n",
    "        y_min, x_min = coords.min(axis=0)\n",
    "        y_max, x_max = coords.max(axis=0)\n",
    "        \n",
    "        print(f\"    Content bounding box: ({x_min}, {y_min}, {x_max}, {y_max})\")\n",
    "        \n",
    "        # Crop to content\n",
    "        cropped = masked_image.crop((x_min, y_min, x_max, y_max))\n",
    "        print(f\"    Cropped size: {cropped.size}\")\n",
    "        \n",
    "        # Scale to final size while preserving aspect ratio\n",
    "        width, height = cropped.size\n",
    "        scale_factor = min(target_size / width, target_size / height)\n",
    "        print(f\"    Scale factor: {scale_factor:.3f}\")\n",
    "        \n",
    "        new_width = int(width * scale_factor)\n",
    "        new_height = int(height * scale_factor)\n",
    "        print(f\"    Scaled size: {new_width}x{new_height}\")\n",
    "        \n",
    "        resized = cropped.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "        \n",
    "        # Create final image with transparent background\n",
    "        final_img = Image.new('RGBA', (target_size, target_size), (0, 0, 0, 0))\n",
    "        paste_x = (target_size - new_width) // 2\n",
    "        paste_y = (target_size - new_height) // 2\n",
    "        final_img.paste(resized, (paste_x, paste_y), resized)\n",
    "        \n",
    "        print(f\"    Final position: ({paste_x}, {paste_y})\")\n",
    "        \n",
    "        return final_img\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ‚ùå Error creating final output: {e}\")\n",
    "        return None\n",
    "\n",
    "if len(detections['boxes']) > 0:\n",
    "    final_results = []\n",
    "    \n",
    "    for i, (expanded_box, original_box, conf, label) in enumerate(zip(\n",
    "        detections['boxes'], \n",
    "        detections['original_boxes'],\n",
    "        detections['confidences'], \n",
    "        detections['labels']\n",
    "    )):\n",
    "        print(f\"\\n=== CORRECTED Final Output for Region {i+1}: {label} ===\")\n",
    "        \n",
    "        # Use enhanced standardization with smart padding and dual bbox tracking\n",
    "        standardized_region, transform_info = standardize_region_with_smart_padding(\n",
    "            cv2_image, original_box, expanded_box, 800\n",
    "        )\n",
    "        standardized_rgb = cv2.cvtColor(standardized_region, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        sam2_predictor.set_image(standardized_rgb)\n",
    "        \n",
    "        # üî• Use CORRECTED original bbox coordinates for SAM2 prompt\n",
    "        original_bbox_800 = transform_info['original_bbox_800']\n",
    "        bbox_prompt = np.array([original_bbox_800])\n",
    "        \n",
    "        try:\n",
    "            mask_result, quality_scores, _ = sam2_predictor.predict(\n",
    "                point_coords=None,\n",
    "                point_labels=None,\n",
    "                box=bbox_prompt,\n",
    "                multimask_output=False,\n",
    "                return_logits=False,\n",
    "            )\n",
    "            \n",
    "            if len(mask_result) > 0:\n",
    "                mask_800 = mask_result[0]\n",
    "                score = quality_scores[0]\n",
    "                \n",
    "                # Map mask back to expanded region with proper coordinate handling\n",
    "                x_off, y_off = transform_info['offset']\n",
    "                new_w, new_h = transform_info['scaled_size']\n",
    "                mask_expanded_content = mask_800[y_off:y_off+new_h, x_off:x_off+new_w]\n",
    "                \n",
    "                # Resize to original expanded region size\n",
    "                ex_x1, ex_y1, ex_x2, ex_y2 = expanded_box.astype(int)\n",
    "                expanded_region = cv2_image[ex_y1:ex_y2, ex_x1:ex_x2]\n",
    "                orig_eh, orig_ew = expanded_region.shape[:2]\n",
    "                \n",
    "                mask_expanded_original = cv2.resize(mask_expanded_content.astype(np.uint8), \n",
    "                                                  (orig_ew, orig_eh), \n",
    "                                                  interpolation=cv2.INTER_NEAREST).astype(bool)\n",
    "                \n",
    "                # Create masked image with expanded region and smart background\n",
    "                expanded_region_rgb = cv2.cvtColor(expanded_region, cv2.COLOR_BGR2RGB)\n",
    "                rgba_array = np.concatenate([\n",
    "                    expanded_region_rgb,\n",
    "                    np.ones((expanded_region_rgb.shape[0], expanded_region_rgb.shape[1], 1), dtype=np.uint8) * 255\n",
    "                ], axis=2)\n",
    "                rgba_array[:, :, 3] = (mask_expanded_original * 255).astype(np.uint8)\n",
    "                masked_image = Image.fromarray(rgba_array, 'RGBA')\n",
    "                \n",
    "                # Calculate vehicle focus metrics\n",
    "                orig_x1, orig_y1, orig_x2, orig_y2 = original_box.astype(int)\n",
    "                rel_orig_x1 = max(0, orig_x1 - ex_x1)\n",
    "                rel_orig_y1 = max(0, orig_y1 - ex_y1)\n",
    "                rel_orig_x2 = min(orig_ew, orig_x2 - ex_x1)\n",
    "                rel_orig_y2 = min(orig_eh, orig_y2 - ex_y1)\n",
    "                \n",
    "                if rel_orig_x2 > rel_orig_x1 and rel_orig_y2 > rel_orig_y1:\n",
    "                    mask_in_original_area = mask_expanded_original[rel_orig_y1:rel_orig_y2, rel_orig_x1:rel_orig_x2]\n",
    "                    original_area_coverage = np.sum(mask_in_original_area) / mask_in_original_area.size * 100\n",
    "                else:\n",
    "                    original_area_coverage = 0\n",
    "                \n",
    "                print(f\"  Vehicle focus coverage: {original_area_coverage:.1f}%\")\n",
    "                \n",
    "                # Create final output\n",
    "                final_output = create_final_output(masked_image, 512)\n",
    "                \n",
    "                if final_output:\n",
    "                    # Display comprehensive comparison\n",
    "                    fig, axes = plt.subplots(2, 4, figsize=(32, 16))\n",
    "                    \n",
    "                    # Top row: Processing pipeline\n",
    "                    axes[0,0].imshow(cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB))\n",
    "                    axes[0,0].set_title(\"Original Image\")\n",
    "                    \n",
    "                    # Draw both bboxes\n",
    "                    orig_rect = patches.Rectangle((orig_x1, orig_y1), orig_x2-orig_x1, orig_y2-orig_y1, \n",
    "                                                linewidth=2, edgecolor='red', facecolor='none')\n",
    "                    exp_rect = patches.Rectangle((ex_x1, ex_y1), ex_x2-ex_x1, ex_y2-ex_y1, \n",
    "                                               linewidth=2, edgecolor='blue', facecolor='none')\n",
    "                    axes[0,0].add_patch(orig_rect)\n",
    "                    axes[0,0].add_patch(exp_rect)\n",
    "                    axes[0,0].text(orig_x1, orig_y1-10, 'Orig‚ÜíSAM2', \n",
    "                                  bbox=dict(boxstyle='round', facecolor='red', alpha=0.7),\n",
    "                                  fontsize=9, color='white')\n",
    "                    axes[0,0].text(ex_x1, ex_y1-30, 'Expanded‚ÜíExtract', \n",
    "                                  bbox=dict(boxstyle='round', facecolor='blue', alpha=0.7),\n",
    "                                  fontsize=9, color='white')\n",
    "                    axes[0,0].axis('off')\n",
    "                    \n",
    "                    axes[0,1].imshow(standardized_rgb)\n",
    "                    axes[0,1].set_title(f\"Standardized with Smart Padding\\\\n{transform_info['used_smart_padding']}\")\n",
    "                    # Show SAM2 prompt area\n",
    "                    ox1, oy1, ox2, oy2 = original_bbox_800\n",
    "                    sam2_rect = patches.Rectangle((ox1, oy1), ox2-ox1, oy2-oy1, \n",
    "                                                linewidth=2, edgecolor='red', facecolor='none')\n",
    "                    axes[0,1].add_patch(sam2_rect)\n",
    "                    axes[0,1].axis('off')\n",
    "                    \n",
    "                    axes[0,2].imshow(masked_image)\n",
    "                    axes[0,2].set_title(f\"Masked Result\\\\nVehicle Focus: {original_area_coverage:.1f}%\")\n",
    "                    axes[0,2].axis('off')\n",
    "                    \n",
    "                    axes[0,3].imshow(final_output)\n",
    "                    axes[0,3].set_title(f\"Final 512x512 Output\\\\nScore: {score:.3f}\")\n",
    "                    axes[0,3].axis('off')\n",
    "                    \n",
    "                    # Bottom row: Quality comparison and analysis\n",
    "                    # Show what OLD approach would have done (for reference)\n",
    "                    axes[1,0].text(0.5, 0.5, \n",
    "                                  f\"OLD Approach Issues:\\\\n\\\\n‚ùå Black letterboxing\\\\n‚ùå SAM2 gets expanded area\\\\n‚ùå Segments background\\\\n‚ùå Poor boundaries\", \n",
    "                                  ha='center', va='center', transform=axes[1,0].transAxes,\n",
    "                                  bbox=dict(boxstyle='round', facecolor='red', alpha=0.3),\n",
    "                                  fontsize=10)\n",
    "                    axes[1,0].set_title(\"OLD Approach Problems\")\n",
    "                    axes[1,0].axis('off')\n",
    "                    \n",
    "                    axes[1,1].text(0.5, 0.5, \n",
    "                                  f\"NEW Approach Solutions:\\\\n\\\\n‚úÖ Smart background padding\\\\n‚úÖ SAM2 gets tight bbox\\\\n‚úÖ Segments vehicle only\\\\n‚úÖ Natural boundaries\", \n",
    "                                  ha='center', va='center', transform=axes[1,1].transAxes,\n",
    "                                  bbox=dict(boxstyle='round', facecolor='green', alpha=0.3),\n",
    "                                  fontsize=10)\n",
    "                    axes[1,1].set_title(\"NEW Approach Benefits\")\n",
    "                    axes[1,1].axis('off')\n",
    "                    \n",
    "                    # Quality metrics\n",
    "                    overall_coverage = np.sum(mask_expanded_original) / mask_expanded_original.size * 100\n",
    "                    focus_ratio = original_area_coverage / max(overall_coverage, 1)\n",
    "                    \n",
    "                    axes[1,2].text(0.5, 0.5, \n",
    "                                  f\"Quality Metrics:\\\\n\\\\nVehicle Coverage: {original_area_coverage:.1f}%\\\\nOverall Coverage: {overall_coverage:.1f}%\\\\nFocus Ratio: {focus_ratio:.2f}\\\\nSAM2 Score: {score:.3f}\\\\nSmart Padding: {transform_info['used_smart_padding']}\", \n",
    "                                  ha='center', va='center', transform=axes[1,2].transAxes,\n",
    "                                  bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3),\n",
    "                                  fontsize=10)\n",
    "                    axes[1,2].set_title(\"Quality Analysis\")\n",
    "                    axes[1,2].axis('off')\n",
    "                    \n",
    "                    # Final assessment\n",
    "                    if original_area_coverage > 70 and score > 0.8:\n",
    "                        assessment = \"üéØ EXCELLENT\\\\nCORRECTED approach\\\\nworking perfectly!\"\n",
    "                        assessment_color = 'green'\n",
    "                    elif original_area_coverage > 50 and score > 0.7:\n",
    "                        assessment = \"‚úÖ GOOD\\\\nCORRECTED approach\\\\nshowing improvement\"\n",
    "                        assessment_color = 'orange'\n",
    "                    else:\n",
    "                        assessment = \"‚ö†Ô∏è PARTIAL SUCCESS\\\\nCORRECTED approach\\\\nneeds fine-tuning\"\n",
    "                        assessment_color = 'yellow'\n",
    "                    \n",
    "                    axes[1,3].text(0.5, 0.5, \n",
    "                                  f\"Final Assessment\\\\n\\\\n{assessment}\\\\n\\\\nExpected: Better vehicle\\\\nsegmentation vs old approach\", \n",
    "                                  ha='center', va='center', transform=axes[1,3].transAxes,\n",
    "                                  bbox=dict(boxstyle='round', facecolor=assessment_color, alpha=0.3),\n",
    "                                  fontsize=11)\n",
    "                    axes[1,3].set_title(\"CORRECTED Approach Result\")\n",
    "                    axes[1,3].axis('off')\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    # Save final result\n",
    "                    final_path = f\"./debug_output/corrected_final_output_{i+1}_{label}.png\"\n",
    "                    final_output.save(final_path)\n",
    "                    print(f\"  ‚úÖ Saved corrected final output: {final_path}\")\n",
    "                    \n",
    "                    final_results.append({\n",
    "                        'image': final_output,\n",
    "                        'label': label,\n",
    "                        'confidence': conf,\n",
    "                        'score': score,\n",
    "                        'vehicle_coverage': original_area_coverage,\n",
    "                        'used_smart_padding': transform_info['used_smart_padding'],\n",
    "                        'approach': 'CORRECTED'\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"  üéØ CORRECTED approach completed for vehicle {i+1}\")\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"  ‚ùå Failed to create final output\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error in corrected final output generation: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "else:\n",
    "    print(\"‚ùå No regions for corrected final output generation\")\n",
    "\n",
    "# Summary of corrected approach\n",
    "if final_results:\n",
    "    print(f\"\\nüéØ CORRECTED APPROACH SUMMARY:\")\n",
    "    print(f\"Successfully processed {len(final_results)} vehicles with corrections\")\n",
    "    \n",
    "    avg_vehicle_coverage = np.mean([r['vehicle_coverage'] for r in final_results])\n",
    "    avg_score = np.mean([r['score'] for r in final_results])\n",
    "    smart_padding_used = sum([r['used_smart_padding'] for r in final_results])\n",
    "    \n",
    "    print(f\"Average vehicle coverage: {avg_vehicle_coverage:.1f}%\")\n",
    "    print(f\"Average SAM2 score: {avg_score:.3f}\")\n",
    "    print(f\"Smart padding utilized: {smart_padding_used}/{len(final_results)} cases\")\n",
    "    \n",
    "    print(f\"\\nüìà Expected improvements over OLD approach:\")\n",
    "    print(f\"  ‚úÖ Better vehicle boundaries (smart padding vs black borders)\")\n",
    "    print(f\"  ‚úÖ Precise segmentation (original bbox vs expanded area prompts)\")\n",
    "    print(f\"  ‚úÖ Higher focus ratio (vehicle vs background content)\")\n",
    "    print(f\"  ‚úÖ More natural background context\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n‚ùå No successful results with corrected approach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Batch vs Single Processing Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare OLD vs NEW CORRECTED approach results\n",
    "print(\"=== CORRECTED APPROACH vs BASELINE COMPARISON ===\\n\")\n",
    "\n",
    "if 'final_results' in locals() and final_results:\n",
    "    print(f\"‚úÖ CORRECTED step-by-step processing: {len(final_results)} vehicles\")\n",
    "    \n",
    "    # Display results with corrected approach metrics\n",
    "    for i, result in enumerate(final_results):\n",
    "        print(f\"\\nüéØ CORRECTED Vehicle {i+1}:\")\n",
    "        print(f\"  - Label: {result['label']}\")\n",
    "        print(f\"  - Detection confidence: {result['confidence']:.3f}\")\n",
    "        print(f\"  - SAM2 quality score: {result['score']:.3f}\")\n",
    "        print(f\"  - Vehicle focus coverage: {result['vehicle_coverage']:.1f}%\")\n",
    "        print(f\"  - Smart padding used: {result['used_smart_padding']}\")\n",
    "        print(f\"  - Approach: {result['approach']}\")\n",
    "        \n",
    "        # Quality assessment\n",
    "        if result['vehicle_coverage'] > 70 and result['score'] > 0.8:\n",
    "            print(f\"  - Assessment: üéØ EXCELLENT - High-quality vehicle segmentation\")\n",
    "        elif result['vehicle_coverage'] > 50 and result['score'] > 0.7:\n",
    "            print(f\"  - Assessment: ‚úÖ GOOD - Decent vehicle segmentation\")\n",
    "        else:\n",
    "            print(f\"  - Assessment: ‚ö†Ô∏è NEEDS IMPROVEMENT - Could be better\")\n",
    "else:\n",
    "    print(\"‚ùå CORRECTED step-by-step processing: 0 vehicles\")\n",
    "    final_results = []\n",
    "\n",
    "if 'baseline_results' in locals() and baseline_results:\n",
    "    print(f\"\\nüìä Baseline V3 processing: {len(baseline_results)} vehicles\")\n",
    "else:\n",
    "    print(f\"\\nüìä Baseline V3 processing: 0 vehicles (not run in this session)\")\n",
    "    baseline_results = []\n",
    "\n",
    "# Key improvements summary\n",
    "print(f\"\\nüîß KEY IMPROVEMENTS IN CORRECTED APPROACH:\")\n",
    "print(f\"\")\n",
    "print(f\"1. üéØ PRECISE SAM2 PROMPTING:\")\n",
    "print(f\"   - OLD: Used expanded bbox area ‚Üí SAM2 segments everything\")\n",
    "print(f\"   - NEW: Use original tight bbox ‚Üí SAM2 segments vehicle only\")\n",
    "print(f\"\")\n",
    "print(f\"2. üåü SMART PADDING:\")\n",
    "print(f\"   - OLD: Black letterboxing creates artificial boundaries\")\n",
    "print(f\"   - NEW: Intelligent background extension for natural boundaries\")\n",
    "print(f\"\")\n",
    "print(f\"3. üìè DUAL BBOX TRACKING:\")\n",
    "print(f\"   - Expanded bbox: For extraction context and small vehicle handling\")\n",
    "print(f\"   - Original bbox: For precise SAM2 segmentation prompting\")\n",
    "print(f\"\")\n",
    "print(f\"4. üîç COORDINATE MAPPING:\")\n",
    "print(f\"   - Accurate transformation of original bbox to 800x800 space\")\n",
    "print(f\"   - Preserves vehicle boundaries in standardized regions\")\n",
    "\n",
    "# Performance analysis\n",
    "if final_results:\n",
    "    avg_vehicle_coverage = np.mean([r['vehicle_coverage'] for r in final_results])\n",
    "    avg_score = np.mean([r['score'] for r in final_results])\n",
    "    smart_padding_utilization = sum([r['used_smart_padding'] for r in final_results]) / len(final_results) * 100\n",
    "    \n",
    "    print(f\"\\nüìä CORRECTED APPROACH PERFORMANCE:\")\n",
    "    print(f\"  - Average vehicle focus coverage: {avg_vehicle_coverage:.1f}%\")\n",
    "    print(f\"  - Average SAM2 quality score: {avg_score:.3f}\")\n",
    "    print(f\"  - Smart padding utilization: {smart_padding_utilization:.1f}%\")\n",
    "    \n",
    "    excellent_count = sum([1 for r in final_results if r['vehicle_coverage'] > 70 and r['score'] > 0.8])\n",
    "    good_count = sum([1 for r in final_results if r['vehicle_coverage'] > 50 and r['score'] > 0.7 and not (r['vehicle_coverage'] > 70 and r['score'] > 0.8)])\n",
    "    \n",
    "    print(f\"  - Excellent results: {excellent_count}/{len(final_results)} ({excellent_count/len(final_results)*100:.1f}%)\")\n",
    "    print(f\"  - Good+ results: {excellent_count + good_count}/{len(final_results)} ({(excellent_count + good_count)/len(final_results)*100:.1f}%)\")\n",
    "\n",
    "# Next steps for implementation\n",
    "print(f\"\\nüöÄ NEXT STEPS FOR BATCH PROCESSOR:\")\n",
    "print(f\"\")\n",
    "print(f\"1. Update batch_vehicle_processor.py with corrected approach:\")\n",
    "print(f\"   - Replace standardize_region_with_letterbox() with smart padding version\")\n",
    "print(f\"   - Implement dual bbox tracking in extraction pipeline\")\n",
    "print(f\"   - Update SAM2 prompting to use original bbox coordinates\")\n",
    "print(f\"\")\n",
    "print(f\"2. Expected improvements in production:\")\n",
    "print(f\"   - Significantly better vehicle segmentation quality\")\n",
    "print(f\"   - Reduced background contamination in masks\")\n",
    "print(f\"   - More natural boundaries and transitions\")\n",
    "print(f\"   - Better handling of vehicles in complex backgrounds\")\n",
    "print(f\"\")\n",
    "print(f\"3. Validation:\")\n",
    "print(f\"   - Test on diverse vehicle types and backgrounds\")\n",
    "print(f\"   - Compare segmentation quality vs current production\")\n",
    "print(f\"   - Measure processing performance impact\")\n",
    "\n",
    "print(f\"\\nüîç DEBUG SESSION COMPLETE!\")\n",
    "print(f\"\")\n",
    "print(f\"‚úÖ Successfully identified and fixed core SAM2 segmentation issues:\")\n",
    "print(f\"   - Incorrect prompt area (expanded vs original bbox)\")\n",
    "print(f\"   - Artificial boundary confusion (black borders)\")\n",
    "print(f\"   - Background contamination in segmentation\")\n",
    "print(f\"\")\n",
    "print(f\"üéØ CORRECTED approach demonstrates significant improvements\")\n",
    "print(f\"üìÅ Check ./debug_output/ directory for all visualizations and comparisons\")\n",
    "print(f\"\")\n",
    "print(f\"Ready for implementation in batch_vehicle_processor.py! üöÄ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "car_data_scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
