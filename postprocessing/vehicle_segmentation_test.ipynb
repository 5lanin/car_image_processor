{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Loading and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SAM2 after installation\n",
    "try:\n",
    "    from sam2.build_sam import build_sam2\n",
    "    from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "    print(\"SAM2 imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"SAM2 import failed: {e}\")\n",
    "    print(\"Please restart the kernel and run the installation cells again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO class definitions (COCO dataset)\n",
    "YOLO_CLASSES = {\n",
    "    0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus',\n",
    "    6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant',\n",
    "    11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat',\n",
    "    16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear',\n",
    "    22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag',\n",
    "    27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard',\n",
    "    32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove',\n",
    "    36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle',\n",
    "    40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl',\n",
    "    46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli',\n",
    "    51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake',\n",
    "    56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table',\n",
    "    61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard',\n",
    "    67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink',\n",
    "    72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors',\n",
    "    77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'\n",
    "}\n",
    "\n",
    "# Vehicle classes we're interested in\n",
    "VEHICLE_CLASSES = {2: 'car', 5: 'bus', 7: 'truck'} # , 3: 'motorcycle'\n",
    "\n",
    "print(f\"Vehicle classes: {VEHICLE_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VehicleProcessor:\n",
    "    def __init__(self, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", \n",
    "                 yolo_conf_threshold=0.25, car_conf_threshold=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the vehicle processing pipeline\n",
    "        \n",
    "        Args:\n",
    "            device: Device to run models on\n",
    "            yolo_conf_threshold: YOLO detection confidence threshold\n",
    "            car_conf_threshold: Minimum confidence for vehicle detections\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.yolo_conf_threshold = yolo_conf_threshold\n",
    "        self.car_conf_threshold = car_conf_threshold\n",
    "        \n",
    "        print(f\"Initializing models on device: {device}\")\n",
    "        \n",
    "        # Load YOLO model\n",
    "        print(\"Loading YOLO model...\")\n",
    "        self.yolo_model = YOLO('yolo11x.pt').to(device)\n",
    "        self.yolo_model.eval()\n",
    "        \n",
    "        # Load SAM2 model\n",
    "        print(\"Loading SAM2 model...\")\n",
    "        checkpoint_path = \"/mnt/damian/Projects/sam2/checkpoints/sam2.1_hiera_large.pt\"\n",
    "        sam2_config = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "        self.sam2_model = build_sam2(sam2_config, checkpoint_path, device=device)\n",
    "        self.sam2_predictor = SAM2ImagePredictor(self.sam2_model)\n",
    "        \n",
    "        print(\"Models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the processor\n",
    "processor = VehicleProcessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Core Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_bbox(box: np.ndarray, image_shape: tuple, expansion_factor: float = 0.15) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Expand bounding box by a factor while keeping it within image bounds\n",
    "    This helps capture the entire vehicle when YOLO detection is partial\n",
    "    \n",
    "    Args:\n",
    "        box: Bounding box [x1, y1, x2, y2]\n",
    "        image_shape: Shape of the image (height, width)\n",
    "        expansion_factor: Factor to expand the box (0.15 = 15% expansion)\n",
    "        \n",
    "    Returns:\n",
    "        Expanded bounding box\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = box\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "    \n",
    "    # Calculate expansion\n",
    "    expand_w = width * expansion_factor\n",
    "    expand_h = height * expansion_factor\n",
    "    \n",
    "    # Apply expansion while staying within image bounds\n",
    "    x1 = max(0, x1 - expand_w / 2)\n",
    "    y1 = max(0, y1 - expand_h / 2)\n",
    "    x2 = min(image_shape[1], x2 + expand_w / 2)\n",
    "    y2 = min(image_shape[0], y2 + expand_h / 2)\n",
    "    \n",
    "    return np.array([x1, y1, x2, y2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_vehicles(processor, image: np.ndarray, bbox_expansion: float = 0.0) -> Dict:\n",
    "    \"\"\"\n",
    "    Detect vehicles in image using YOLO with optional bounding box expansion\n",
    "    \n",
    "    Args:\n",
    "        processor: VehicleProcessor instance\n",
    "        image: Input image as numpy array\n",
    "        bbox_expansion: Factor to expand bounding boxes, default 0.0, example: 0.15 = 15%\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with detection results\n",
    "    \"\"\"\n",
    "    results = processor.yolo_model(source=image, conf=processor.yolo_conf_threshold, verbose=False)\n",
    "    \n",
    "    detections = sv.Detections.from_ultralytics(results[0])\n",
    "    \n",
    "    # Filter for vehicle classes only\n",
    "    vehicle_mask = np.isin(detections.class_id, list(VEHICLE_CLASSES.keys()))\n",
    "    \n",
    "    if vehicle_mask.any():\n",
    "        # Expand bounding boxes to capture full vehicle\n",
    "        expanded_boxes = []\n",
    "        for box in detections.xyxy[vehicle_mask]:\n",
    "            if bbox_expansion > 0:\n",
    "                expanded_box = expand_bbox(box, image.shape[:2], bbox_expansion)\n",
    "            else:\n",
    "                expanded_box = box\n",
    "            expanded_boxes.append(expanded_box)\n",
    "        \n",
    "        filtered_detections = {\n",
    "            'boxes': np.array(expanded_boxes),\n",
    "            'original_boxes': detections.xyxy[vehicle_mask],  # Keep original for reference\n",
    "            'confidences': detections.confidence[vehicle_mask], \n",
    "            'class_ids': detections.class_id[vehicle_mask],\n",
    "            'labels': [YOLO_CLASSES[class_id] for class_id in detections.class_id[vehicle_mask]]\n",
    "        }\n",
    "    else:\n",
    "        filtered_detections = {\n",
    "            'boxes': np.array([]),\n",
    "            'original_boxes': np.array([]),\n",
    "            'confidences': np.array([]),\n",
    "            'class_ids': np.array([]),\n",
    "            'labels': []\n",
    "        }\n",
    "    \n",
    "    return filtered_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_vehicles_sam2(processor, image: np.ndarray, detections: Dict) -> Tuple[List[np.ndarray], List[float]]:\n",
    "    \"\"\"\n",
    "    Use SAM2 to segment vehicles based on YOLO detections\n",
    "    \n",
    "    Args:\n",
    "        processor: VehicleProcessor instance\n",
    "        image: Input image as numpy array\n",
    "        detections: YOLO detection results\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (masks, quality_scores)\n",
    "    \"\"\"\n",
    "    if len(detections['boxes']) == 0:\n",
    "        return [], []\n",
    "    \n",
    "    # Set image in SAM2 predictor\n",
    "    processor.sam2_predictor.set_image(image)\n",
    "    \n",
    "    masks = []\n",
    "    scores = []\n",
    "    \n",
    "    # Process each detected vehicle\n",
    "    for i, (box, confidence, class_id) in enumerate(zip(\n",
    "        detections['boxes'], \n",
    "        detections['confidences'], \n",
    "        detections['class_ids']\n",
    "    )):\n",
    "        # Only process high-confidence vehicle detections\n",
    "        if confidence < processor.car_conf_threshold:\n",
    "            continue\n",
    "        \n",
    "        # Convert box to SAM2 format (xyxy)\n",
    "        box_prompt = box.reshape(1, 4)  # SAM expects (N, 4)\n",
    "        \n",
    "        # Get segmentation mask\n",
    "        mask_result, quality_scores, _ = processor.sam2_predictor.predict(\n",
    "            point_coords=None,\n",
    "            point_labels=None,\n",
    "            box=box_prompt,\n",
    "            multimask_output=False,\n",
    "            return_logits=False,\n",
    "        )\n",
    "        \n",
    "        if len(mask_result) > 0:\n",
    "            masks.append(mask_result[0])  # Take the first (and only) mask\n",
    "            scores.append(quality_scores[0])\n",
    "    \n",
    "    return masks, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask_and_rescale(pil_image: Image.Image, mask: np.ndarray, target_size: int = 512, \n",
    "                           min_vehicle_size: int = 256, max_upscale_factor: float = 2.0) -> Optional[Image.Image]:\n",
    "    \"\"\"\n",
    "    Apply SAM2 mask to image and adaptively rescale based on vehicle size\n",
    "    \n",
    "    Args:\n",
    "        pil_image: Input PIL image\n",
    "        mask: Binary mask from SAM2\n",
    "        target_size: Target image size (will be square)\n",
    "        min_vehicle_size: Minimum size before applying limited upscaling\n",
    "        max_upscale_factor: Maximum upscaling without super resolution\n",
    "        \n",
    "    Returns:\n",
    "        Processed image with transparent background\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert PIL to numpy\n",
    "        img_array = np.array(pil_image)\n",
    "        \n",
    "        # Create RGBA image\n",
    "        if img_array.shape[2] == 3:\n",
    "            rgba_array = np.concatenate([\n",
    "                img_array, \n",
    "                np.ones((img_array.shape[0], img_array.shape[1], 1), dtype=np.uint8) * 255\n",
    "            ], axis=2)\n",
    "        else:\n",
    "            rgba_array = img_array.copy()\n",
    "        \n",
    "        # Apply mask to alpha channel\n",
    "        rgba_array[:, :, 3] = (mask * 255).astype(np.uint8)\n",
    "        \n",
    "        # Convert back to PIL\n",
    "        masked_image = Image.fromarray(rgba_array, 'RGBA')\n",
    "        \n",
    "        # Find bounding box of non-transparent pixels\n",
    "        alpha = np.array(masked_image.getchannel('A'))\n",
    "        coords = np.argwhere(alpha > 0)\n",
    "        \n",
    "        if len(coords) == 0:\n",
    "            return None\n",
    "        \n",
    "        y_min, x_min = coords.min(axis=0)\n",
    "        y_max, x_max = coords.max(axis=0)\n",
    "        \n",
    "        # Crop to content\n",
    "        cropped = masked_image.crop((x_min, y_min, x_max, y_max))\n",
    "        \n",
    "        # Get original vehicle dimensions\n",
    "        original_width, original_height = cropped.size\n",
    "        original_size = max(original_width, original_height)\n",
    "        \n",
    "        # Determine scaling strategy based on vehicle size\n",
    "        if original_size >= target_size:\n",
    "            # Vehicle is large enough, scale down to fit\n",
    "            scale_factor = target_size / original_size\n",
    "            print(f\"  → Large vehicle ({original_size}px), downscaling\")\n",
    "        elif original_size >= min_vehicle_size:\n",
    "            # Medium size: scale to fit without exceeding target\n",
    "            scale_factor = min(target_size / original_size, max_upscale_factor)\n",
    "            print(f\"  → Medium vehicle ({original_size}px), moderate upscaling\")\n",
    "        else:\n",
    "            # Small vehicle: limited upscaling to avoid quality loss\n",
    "            scale_factor = min(max_upscale_factor, target_size / original_size)\n",
    "            print(f\"  → Small vehicle ({original_size}px), limited upscaling (max {max_upscale_factor}x)\")\n",
    "        \n",
    "        # Apply scaling\n",
    "        new_width = int(original_width * scale_factor)\n",
    "        new_height = int(original_height * scale_factor)\n",
    "        \n",
    "        resized = cropped.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "        \n",
    "        # Create final image with transparent background\n",
    "        final_img = Image.new('RGBA', (target_size, target_size), (0, 0, 0, 0))\n",
    "        paste_x = (target_size - new_width) // 2\n",
    "        paste_y = (target_size - new_height) // 2\n",
    "        final_img.paste(resized, (paste_x, paste_y), resized)\n",
    "        \n",
    "        print(f\"  → Final size in canvas: {new_width}x{new_height} (scale: {scale_factor:.2f}x)\")\n",
    "        \n",
    "        return final_img\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error applying mask: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detections(image: np.ndarray, detections: Dict, figsize=(12, 8)):\n",
    "    \"\"\"\n",
    "    Visualize YOLO detections with bounding boxes\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    \n",
    "    # Convert BGR to RGB for display\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "        display_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    else:\n",
    "        display_image = image\n",
    "    \n",
    "    ax.imshow(display_image)\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    for i, (box, conf, label) in enumerate(zip(\n",
    "        detections['boxes'], detections['confidences'], detections['labels']\n",
    "    )):\n",
    "        x1, y1, x2, y2 = box\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        \n",
    "        # Create rectangle\n",
    "        rect = patches.Rectangle((x1, y1), width, height, \n",
    "                               linewidth=2, edgecolor='red', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add label\n",
    "        ax.text(x1, y1-10, f'{label}: {conf:.2f}', \n",
    "               bbox=dict(boxstyle='round', facecolor='red', alpha=0.7),\n",
    "               fontsize=10, color='white')\n",
    "    \n",
    "    ax.set_title(f'Vehicle Detections ({len(detections[\"boxes\"])} found)')\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_masks(image: np.ndarray, masks: List[np.ndarray], detections: Dict, figsize=(15, 5)):\n",
    "    \"\"\"\n",
    "    Visualize SAM2 segmentation masks\n",
    "    \"\"\"\n",
    "    if not masks:\n",
    "        print(\"No masks to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Convert BGR to RGB for display\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "        display_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    else:\n",
    "        display_image = image\n",
    "    \n",
    "    num_masks = len(masks)\n",
    "    fig, axes = plt.subplots(1, min(num_masks + 1, 4), figsize=figsize)\n",
    "    \n",
    "    if num_masks == 0:\n",
    "        axes = [axes]\n",
    "    elif num_masks + 1 == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Show original image\n",
    "    axes[0].imshow(display_image)\n",
    "    axes[0].set_title('Original')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Show each mask\n",
    "    for i, mask in enumerate(masks[:3]):  # Show up to 3 masks\n",
    "        # Convert mask to boolean type for proper indexing\n",
    "        mask_bool = mask.astype(bool)\n",
    "        \n",
    "        # Overlay mask on image\n",
    "        overlay = display_image.copy().astype(np.float32)\n",
    "        overlay[mask_bool] = overlay[mask_bool] * 0.5 + np.array([255, 0, 0]) * 0.5\n",
    "        overlay = overlay.astype(np.uint8)\n",
    "        \n",
    "        axes[i+1].imshow(overlay)\n",
    "        conf = detections['confidences'][i] if i < len(detections['confidences']) else 0\n",
    "        label = detections['labels'][i] if i < len(detections['labels']) else 'vehicle'\n",
    "        axes[i+1].set_title(f'Mask {i+1}: {label} ({conf:.2f})')\n",
    "        axes[i+1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_final_results(processed_images: List[Image.Image], figsize=(15, 5)):\n",
    "    \"\"\"\n",
    "    Visualize final processed vehicle images\n",
    "    \"\"\"\n",
    "    if not processed_images:\n",
    "        print(\"No processed images to visualize\")\n",
    "        return\n",
    "    \n",
    "    num_images = len(processed_images)\n",
    "    fig, axes = plt.subplots(1, min(num_images, 4), figsize=figsize)\n",
    "    \n",
    "    if num_images == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, img in enumerate(processed_images[:4]):  # Show up to 4 images\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f'Vehicle {i+1} (512x512)')\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "        # Add grid to show 512x512 boundaries\n",
    "        axes[i].set_xlim(0, 512)\n",
    "        axes[i].set_ylim(512, 0)\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_image(image_path: str, processor: VehicleProcessor, \n",
    "                        min_segmentation_score: float = 0.7,\n",
    "                        target_size: int = 512,\n",
    "                        visualize: bool = True) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Complete processing pipeline for a single image\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        processor: VehicleProcessor instance\n",
    "        min_segmentation_score: Minimum SAM2 quality score\n",
    "        target_size: Target output image size\n",
    "        visualize: Whether to show visualizations\n",
    "        \n",
    "    Returns:\n",
    "        List of processed vehicle data\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    print(f\"Processing: {image_path}\")\n",
    "    \n",
    "    # Load as PIL image\n",
    "    pil_image = Image.open(image_path).convert('RGB')\n",
    "    print(f\"Original image size: {pil_image.size}\")\n",
    "    \n",
    "    # Convert to OpenCV format for processing\n",
    "    cv2_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Step 1: Detect vehicles with YOLO\n",
    "    print(\"Step 1: Running YOLO detection...\")\n",
    "    detections = detect_vehicles(processor, cv2_image)\n",
    "    \n",
    "    if len(detections['boxes']) == 0:\n",
    "        print(\"No vehicles detected!\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Found {len(detections['boxes'])} vehicles: {detections['labels']}\")\n",
    "    \n",
    "    if visualize:\n",
    "        visualize_detections(cv2_image, detections)\n",
    "    \n",
    "    # Step 2: Segment vehicles with SAM2\n",
    "    print(\"Step 2: Running SAM2 segmentation...\")\n",
    "    masks, mask_scores = segment_vehicles_sam2(processor, cv2_image, detections)\n",
    "    \n",
    "    if not masks:\n",
    "        print(\"No valid masks generated!\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Generated {len(masks)} masks with scores: {[f'{s:.3f}' for s in mask_scores]}\")\n",
    "    \n",
    "    if visualize:\n",
    "        visualize_masks(cv2_image, masks, detections)\n",
    "    \n",
    "    # Step 3: Apply masks and rescale\n",
    "    print(\"Step 3: Applying masks and rescaling...\")\n",
    "    processed_results = []\n",
    "    processed_images = []\n",
    "    \n",
    "    for i, (mask, score, confidence, label) in enumerate(\n",
    "        zip(masks, mask_scores, detections['confidences'], detections['labels'])\n",
    "    ):\n",
    "        if score < min_segmentation_score:\n",
    "            print(f\"Skipping mask {i} with low score: {score:.3f}\")\n",
    "            continue\n",
    "        \n",
    "        # Apply mask and rescale\n",
    "        processed_image = apply_mask_and_rescale(pil_image, mask, target_size)\n",
    "        \n",
    "        if processed_image is not None:\n",
    "            result = {\n",
    "                'processed_image': processed_image,\n",
    "                'detection_label': label,\n",
    "                'detection_confidence': float(confidence),\n",
    "                'segmentation_score': float(score),\n",
    "                'mask_index': i,\n",
    "                'original_path': image_path\n",
    "            }\n",
    "            processed_results.append(result)\n",
    "            processed_images.append(processed_image)\n",
    "            print(f\"Successfully processed {label} (conf: {confidence:.3f}, seg: {score:.3f})\")\n",
    "    \n",
    "    if visualize and processed_images:\n",
    "        visualize_final_results(processed_images)\n",
    "    \n",
    "    print(f\"Processing complete. Generated {len(processed_results)} vehicle images.\")\n",
    "    return processed_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test with Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test image path - using one of the car images from your dataset\n",
    "test_image_path = \"/mnt/damian/Projects/car_data_scraper/images/autoevolution_renderings/article_230605/230605_reborn-ford-bronco-ii-morphs-ranger-ms-rt-dna-to-mix-and-match-with-suv-body-style_7_15.jpg\"\n",
    "\n",
    "# Verify the image exists\n",
    "if os.path.exists(test_image_path):\n",
    "    print(f\"Test image found: {test_image_path}\")\n",
    "    \n",
    "    # Show the original image first\n",
    "    orig_img = Image.open(test_image_path)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(orig_img)\n",
    "    plt.title(f\"Original Test Image ({orig_img.size[0]}x{orig_img.size[1]})\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(f\"Test image not found: {test_image_path}\")\n",
    "    print(\"Please check the path or choose a different image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete processing pipeline\n",
    "if os.path.exists(test_image_path):\n",
    "    results = process_single_image(\n",
    "        image_path=test_image_path,\n",
    "        processor=processor,\n",
    "        min_segmentation_score=0.6,  # Lower threshold for testing\n",
    "        target_size=512,\n",
    "        visualize=True\n",
    "    )\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(f\"\\n=== PROCESSING RESULTS ===\")\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"Vehicle {i+1}:\")\n",
    "        print(f\"  - Label: {result['detection_label']}\")\n",
    "        print(f\"  - Detection Confidence: {result['detection_confidence']:.3f}\")\n",
    "        print(f\"  - Segmentation Score: {result['segmentation_score']:.3f}\")\n",
    "        print(f\"  - Output Size: {result['processed_image'].size}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed images if desired\n",
    "save_results = True  # Set to True to save images\n",
    "\n",
    "if save_results and 'results' in locals() and results:\n",
    "    output_dir = \"./test_output\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        filename = f\"processed_vehicle_{i+1}_{result['detection_label']}.png\"\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        \n",
    "        result['processed_image'].save(output_path)\n",
    "        print(f\"Saved: {output_path}\")\n",
    "    \n",
    "    print(f\"\\nAll processed images saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Quality Metrics and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(results: List[Dict]):\n",
    "    \"\"\"\n",
    "    Analyze the quality and characteristics of processed results\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to analyze\")\n",
    "        return\n",
    "    \n",
    "    print(f\"=== QUALITY ANALYSIS ===\")\n",
    "    print(f\"Total vehicles processed: {len(results)}\")\n",
    "    \n",
    "    # Detection confidence statistics\n",
    "    det_confs = [r['detection_confidence'] for r in results]\n",
    "    print(f\"\\nDetection Confidence:\")\n",
    "    print(f\"  - Min: {min(det_confs):.3f}\")\n",
    "    print(f\"  - Max: {max(det_confs):.3f}\")\n",
    "    print(f\"  - Average: {np.mean(det_confs):.3f}\")\n",
    "    \n",
    "    # Segmentation score statistics\n",
    "    seg_scores = [r['segmentation_score'] for r in results]\n",
    "    print(f\"\\nSegmentation Quality:\")\n",
    "    print(f\"  - Min: {min(seg_scores):.3f}\")\n",
    "    print(f\"  - Max: {max(seg_scores):.3f}\")\n",
    "    print(f\"  - Average: {np.mean(seg_scores):.3f}\")\n",
    "    \n",
    "    # Vehicle type distribution\n",
    "    labels = [r['detection_label'] for r in results]\n",
    "    from collections import Counter\n",
    "    label_counts = Counter(labels)\n",
    "    \n",
    "    print(f\"\\nVehicle Types:\")\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"  - {label}: {count}\")\n",
    "\n",
    "# Analyze our test results\n",
    "if 'results' in locals():\n",
    "    analyze_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test enhanced processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-SAM Super Resolution Approach (v3)\n",
    "\n",
    "**Problem with Previous Approach**: Applying super resolution after SAM segmentation caused border artifacts because SR models are trained on natural images, not masked content.\n",
    "\n",
    "**Improved Solution**: Apply super resolution to bounding box regions BEFORE SAM segmentation.\n",
    "\n",
    "### New Workflow:\n",
    "1. **Detect** vehicles with YOLO\n",
    "2. **Extract** bounding box regions with padding\n",
    "3. **Apply SR** to small vehicle regions (natural image content)\n",
    "4. **Segment** with SAM2 on upscaled regions\n",
    "5. **Apply mask** and final scaling\n",
    "\n",
    "### Key Benefits:\n",
    "- ✅ **Cleaner borders**: SR works on natural image content\n",
    "- ✅ **Better segmentation**: SAM2 operates on higher resolution input\n",
    "- ✅ **Compute efficient**: Only upscale regions that need it\n",
    "- ✅ **Quality preservation**: No artifacts from masked SR\n",
    "\n",
    "### Usage Example:\n",
    "```python\n",
    "from enhanced_vehicle_processor_v3 import PreSAMSuperResolutionProcessor\n",
    "\n",
    "processor = PreSAMSuperResolutionProcessor(\n",
    "    enable_super_resolution=True,\n",
    "    sr_threshold_size=300,  # Apply SR if bbox < 300px\n",
    "    bbox_padding=0.25,      # 25% padding for extraction\n",
    "    bbox_expansion=0.15     # 15% bbox expansion for detection\n",
    ")\n",
    "\n",
    "results = processor.process_image(image_path)\n",
    "```\n",
    "\n",
    "### Quality Comparison:\n",
    "- **Post-SAM SR (v2)**: Good vehicle capture, some border artifacts\n",
    "- **Pre-SAM SR (v3)**: Excellent vehicle capture, clean natural borders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "from enhanced_vehicle_processor import PreSAMSuperResolutionProcessor\n",
    "\n",
    "# Initialize processor\n",
    "processor = PreSAMSuperResolutionProcessor(\n",
    "    enable_super_resolution=True,\n",
    "    sr_threshold_size=500,  # Test with higher threshold\n",
    "    max_upscale_factor=2.0,\n",
    "    bbox_expansion=0.05,\n",
    "    bbox_padding=0.10  # Extra padding for SR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = \"/mnt/damian/Projects/car_data_scraper/images/autoevolution_renderings/article_230605/230605_reborn-ford-bronco-ii-morphs-ranger-ms-rt-dna-to-mix-and-match-with-suv-body-style_6_10.jpg\"\n",
    "\n",
    "# Process\n",
    "results = processor.process_image(\n",
    "    test_image_path,\n",
    "    min_segmentation_score=0.6,\n",
    "    target_size=512\n",
    ")\n",
    "    \n",
    "# Display and save results\n",
    "if results:\n",
    "    print(f\"\\n=== RESULTS ===\")\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = \"./test_output\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Process each result\n",
    "    for i, result in enumerate(results):\n",
    "        metrics = result['processing_metrics']\n",
    "        \n",
    "        print(f\"\\nVehicle {i+1}:\")\n",
    "        print(f\"  Label: {metrics['detection_label']}\")\n",
    "        print(f\"  Original size: {metrics['original_vehicle_size']}px\")\n",
    "        print(f\"  Used SR: {metrics['used_super_resolution']}\")\n",
    "        print(f\"  SR scale: {metrics.get('sr_scale_factor', 1.0):.2f}x\")\n",
    "        print(f\"  Segmentation score: {metrics['segmentation_score']:.3f}\")\n",
    "        print(f\"  Final size: {metrics['final_size']}\")\n",
    "        \n",
    "        # Save image\n",
    "        filename = f\"pre_sam_sr_vehicle_{i+1}_{metrics['detection_label']}.png\"\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        result['processed_image'].save(output_path)\n",
    "        print(f\"  Saved: {output_path}\")\n",
    "    \n",
    "    # Create comparison plot\n",
    "    fig, axes = plt.subplots(1, len(results), figsize=(5 * len(results), 5))\n",
    "    if len(results) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, result in zip(axes, results):\n",
    "        ax.imshow(result['processed_image'])\n",
    "        metrics = result['processing_metrics']\n",
    "        title = f\"{metrics['detection_label']}\\n\"\n",
    "        title += f\"Original: {metrics['original_vehicle_size']}px\\n\"\n",
    "        title += f\"Pre-SAM SR: {'Yes' if metrics['used_super_resolution'] else 'No'}\\n\"\n",
    "        title += f\"Seg Score: {metrics['segmentation_score']:.3f}\"\n",
    "        ax.set_title(title, fontsize=10)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"pre_sam_sr_comparison.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No vehicles processed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Compare to rembg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from rembg import remove\n",
    "\n",
    "def remove_background_and_rescale(img, target_size=512):\n",
    "    img_no_bg = remove(img)\n",
    "    alpha = np.array(img_no_bg.getchannel('A'))\n",
    "    coords = np.argwhere(alpha > 0)\n",
    "    if len(coords) == 0:\n",
    "        return None\n",
    "    y_min, x_min = coords.min(axis=0)\n",
    "    y_max, x_max = coords.max(axis=0)\n",
    "    cropped = img_no_bg.crop((x_min, y_min, x_max, y_max))\n",
    "    width, height = cropped.size\n",
    "    scale_factor = min(target_size / width, target_size / height)\n",
    "    new_width = int(width * scale_factor)\n",
    "    new_height = int(height * scale_factor)\n",
    "    resized = cropped.resize((new_width, new_height), Image.LANCZOS)\n",
    "    final_img = Image.new('RGBA', (target_size, target_size), (0, 0, 0, 0))\n",
    "    paste_x = (target_size - new_width) // 2\n",
    "    paste_y = (target_size - new_height) // 2\n",
    "    final_img.paste(resized, (paste_x, paste_y), resized)\n",
    "    return final_img\n",
    "\n",
    "test_image = Image.open(test_image_path).convert('RGBA')\n",
    "processed_image = remove_background_and_rescale(test_image)\n",
    "processed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion and Next Steps\n",
    "\n",
    "This notebook demonstrates the complete YOLO + SAM2 pipeline for vehicle image processing. The workflow successfully:\n",
    "\n",
    "1. ✅ Detects vehicles using YOLO11 with configurable confidence thresholds\n",
    "2. ✅ Segments vehicles precisely using SAM2 with bounding box prompts\n",
    "3. ✅ Applies quality filtering based on detection and segmentation scores\n",
    "4. ✅ Removes backgrounds and rescales to 512x512 while preserving aspect ratio\n",
    "5. ✅ Provides comprehensive visualization at each step\n",
    "\n",
    "### Key Advantages Over Generic Background Removal:\n",
    "- **Vehicle-specific targeting**: Only processes detected vehicles\n",
    "- **Higher precision**: SAM2 provides cleaner edges than rembg\n",
    "- **Quality metrics**: Confidence scores for filtering\n",
    "- **Batch processing ready**: Can be scaled for your full dataset\n",
    "\n",
    "### Next Steps:\n",
    "1. **Tune thresholds** based on your specific dataset requirements\n",
    "2. **Scale to batch processing** using the enhanced_car_processor.py framework\n",
    "3. **Optimize performance** by choosing appropriate SAM2 model size\n",
    "4. **Add data augmentation** if needed for your text-to-image training\n",
    "\n",
    "The pipeline is now ready for integration with your larger dataset processing workflow!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "car_data_scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
